{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide & Deep Recommender System\n",
    "\n",
    "#### Students Group Number: 1\n",
    "#### Students Name and ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Embeddings\n",
    "    embedding_dim: int = 32\n",
    "    embedding_dropout: float = 0.25  # Moderate dropout\n",
    "    \n",
    "    # Deep tower - Shallower [256, 128]\n",
    "    deep_layers: Tuple[int, ...] = (256, 128)  # Removed 3rd layer\n",
    "    deep_dropout: float = 0.35  # Moderate dropout\n",
    "    \n",
    "    # Wide tower\n",
    "    wide_hidden_dim: int = 32\n",
    "    wide_dropout: float = 0.35  # Moderate dropout\n",
    "    \n",
    "    # Biases\n",
    "    bias_dropout: float = 0.1\n",
    "    \n",
    "    # Optimizer - Moderate weight decay\n",
    "    lr: float = 5e-4\n",
    "    weight_decay: float = 5e-3  # Moderate weight decay (0.005)\n",
    "    \n",
    "    # Scheduler\n",
    "    warmup_epochs: int = 2\n",
    "    scheduler_factor: float = 0.5\n",
    "    scheduler_patience: int = 3\n",
    "    min_lr: float = 1e-5\n",
    "    \n",
    "    # Training\n",
    "    n_epochs: int = 25\n",
    "    batch_size: int = 2048\n",
    "    patience: int = 6\n",
    "    grad_clip: float = 0.9\n",
    "    \n",
    "    # Features - Standard smoothing (no adaptive)\n",
    "    smoothing_strength: float = 80.0  # Standard Bayesian smoothing\n",
    "    max_tags: int = 80\n",
    "    \n",
    "    # Bucketing thresholds\n",
    "    user_activity_thresholds: Tuple[int, ...] = (10, 40, 100)\n",
    "    item_popularity_thresholds: Tuple[int, ...] = (30, 150, 400)\n",
    "    \n",
    "    # Ensemble\n",
    "    seeds: Tuple[int, ...] = (42, 123, 456, 789, 2025)\n",
    "    n_seeds_to_use: int = 5\n",
    "    \n",
    "    # Larger validation set for better estimate\n",
    "    val_ratio: float = 0.25  # 25% validation\n",
    "    final_val_ratio: float = 0.05  # For retrain phase\n",
    "\n",
    "\n",
    "CONFIG = Config()\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using MPS (Apple Silicon GPU)\n",
      "PyTorch: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"✓ Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"✓ Using CUDA GPU\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: recsys-runi-2026\n"
     ]
    }
   ],
   "source": [
    "# === DATA PATHS ===\n",
    "DATA_DIR = \"../project/recsys-runi-2026\"\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    DATA_DIR = \"recsys-runi-2026\"\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    DATA_DIR = \".\"\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tag(tag: str) -> str:\n",
    "    if pd.isna(tag):\n",
    "        return \"\"\n",
    "    tag = str(tag).lower()\n",
    "    tag = re.sub(r'[^a-z0-9\\s]', '', tag)\n",
    "    tag = re.sub(r'\\s+', ' ', tag)\n",
    "    return tag.strip()\n",
    "\n",
    "\n",
    "def extract_movie_year(title: str) -> Tuple[str, Optional[int]]:\n",
    "    if pd.isna(title):\n",
    "        return \"\", None\n",
    "    match = re.search(r'\\((\\d{4})(?:-\\d{4})?\\)\\s*$', title)\n",
    "    if match:\n",
    "        return title.strip(), int(match.group(1))\n",
    "    return title.strip(), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_coldstart_split(df: pd.DataFrame, val_user_ratio: float = 0.15, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split by USERS - hold out val_user_ratio% of users entirely.\n",
    "    This prevents leakage by ensuring validation users are completely unseen during training.\n",
    "    Simulates cold-start scenario which better matches test distribution.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[USER COLD-START SPLIT] Holding out {val_user_ratio*100:.0f}% of users...\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get all unique users\n",
    "    all_users = df['user_id'].unique()\n",
    "    n_users = len(all_users)\n",
    "    n_val_users = int(n_users * val_user_ratio)\n",
    "    \n",
    "    # Shuffle and split users\n",
    "    np.random.shuffle(all_users)\n",
    "    val_users = set(all_users[:n_val_users])\n",
    "    train_users = set(all_users[n_val_users:])\n",
    "    \n",
    "    # Split dataframe by users\n",
    "    train_df = df[df['user_id'].isin(train_users)].reset_index(drop=True)\n",
    "    val_df = df[df['user_id'].isin(val_users)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Train users: {len(train_users):,}, Train ratings: {len(train_df):,}\")\n",
    "    print(f\"  Val users: {len(val_users):,}, Val ratings: {len(val_df):,}\")\n",
    "    print(f\"  Avg ratings per train user: {len(train_df)/len(train_users):.1f}\")\n",
    "    print(f\"  Avg ratings per val user: {len(val_df)/len(val_users):.1f}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def item_coldstart_split(df: pd.DataFrame, val_item_ratio: float = 0.10, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split by ITEMS - hold out val_item_ratio% of movies entirely.\n",
    "    This simulates cold-start scenario for new items not seen during training.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[ITEM COLD-START SPLIT] Holding out {val_item_ratio*100:.0f}% of items...\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get all unique items\n",
    "    all_items = df['movie_id'].unique()\n",
    "    n_items = len(all_items)\n",
    "    n_val_items = int(n_items * val_item_ratio)\n",
    "    \n",
    "    # Shuffle and split items\n",
    "    np.random.shuffle(all_items)\n",
    "    val_items = set(all_items[:n_val_items])\n",
    "    train_items = set(all_items[n_val_items:])\n",
    "    \n",
    "    # Split dataframe by items\n",
    "    train_df = df[df['movie_id'].isin(train_items)].reset_index(drop=True)\n",
    "    val_df = df[df['movie_id'].isin(val_items)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Train items: {len(train_items):,}, Train ratings: {len(train_df):,}\")\n",
    "    print(f\"  Val items (cold): {len(val_items):,}, Val ratings: {len(val_df):,}\")\n",
    "    print(f\"  Avg ratings per train item: {len(train_df)/len(train_items):.1f}\")\n",
    "    print(f\"  Avg ratings per val item: {len(val_df)/len(val_items):.1f}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "def stratified_rating_split(df: pd.DataFrame, val_ratio: float = 0.05, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Stratified split for retrain phase - take val_ratio of each user's ratings.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    train_indices, val_indices = [], []\n",
    "    \n",
    "    for user_id, group in df.groupby('user_id'):\n",
    "        indices = group.index.tolist()\n",
    "        np.random.shuffle(indices)\n",
    "        n_val = max(1, int(len(indices) * val_ratio))\n",
    "        train_indices.extend(indices[n_val:])\n",
    "        val_indices.extend(indices[:n_val])\n",
    "    \n",
    "    return df.loc[train_indices].reset_index(drop=True), df.loc[val_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature store initialized.\n"
     ]
    }
   ],
   "source": [
    "class FeatureStore:\n",
    "    \"\"\"\n",
    "    Simplified feature store - no adaptive smoothing, no complex stats.\n",
    "    Target encoding computed ONLY on training data to prevent leakage.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.user_id_to_idx = {}\n",
    "        self.item_id_to_idx = {}\n",
    "        self.n_users = 0\n",
    "        self.n_items = 0\n",
    "        self.genre_list = []\n",
    "        self.genre_features = {}\n",
    "        self.movie_years = {}\n",
    "        self.movie_year_bucket = {}\n",
    "        \n",
    "        # Standard target encoding (computed on train only)\n",
    "        self.user_mean_rating = {}\n",
    "        self.item_mean_rating = {}\n",
    "        self.user_rating_count = {}\n",
    "        self.item_rating_count = {}\n",
    "        self.user_activity_bucket = {}\n",
    "        self.item_popularity_bucket = {}\n",
    "        \n",
    "        self.global_mean = 3.5\n",
    "    \n",
    "    def build_basic(self, train_df, submission_df, movies_df):\n",
    "        \"\"\"Build ID mappings and movie metadata.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BUILDING FEATURE STORE (Temporal Generalization)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ID mappings (include all users/items from train + submission)\n",
    "        print(\"[1/3] Building ID mappings...\")\n",
    "        all_users = set(train_df['user_id'].unique()) | set(submission_df['user_id'].unique())\n",
    "        all_items = set(train_df['movie_id'].unique()) | set(submission_df['movie_id'].unique()) | set(movies_df['movie_id'].unique())\n",
    "        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(sorted(all_users))}\n",
    "        self.item_id_to_idx = {iid: idx for idx, iid in enumerate(sorted(all_items))}\n",
    "        self.n_users = len(self.user_id_to_idx)\n",
    "        self.n_items = len(self.item_id_to_idx)\n",
    "        print(f\"  Users: {self.n_users:,}, Items: {self.n_items:,}\")\n",
    "        \n",
    "        # Movie metadata\n",
    "        print(\"[2/3] Extracting movie metadata...\")\n",
    "        for _, row in movies_df.iterrows():\n",
    "            mid = row['movie_id']\n",
    "            _, year = extract_movie_year(row['title'])\n",
    "            self.movie_years[mid] = year\n",
    "            if year:\n",
    "                if year < 1970: self.movie_year_bucket[mid] = 0\n",
    "                elif year >= 2010: self.movie_year_bucket[mid] = 5\n",
    "                else: self.movie_year_bucket[mid] = min(5, (year - 1970) // 10 + 1)\n",
    "            else:\n",
    "                self.movie_year_bucket[mid] = 3\n",
    "        \n",
    "        # Genre features\n",
    "        print(\"[3/3] Building genre features...\")\n",
    "        all_genres = set()\n",
    "        for g in movies_df['genres'].dropna():\n",
    "            if g != '(no genres listed)':\n",
    "                all_genres.update(g.split('|'))\n",
    "        self.genre_list = sorted(list(all_genres))\n",
    "        \n",
    "        for _, row in movies_df.iterrows():\n",
    "            mid = row['movie_id']\n",
    "            genres = row['genres'].split('|') if pd.notna(row['genres']) and row['genres'] != '(no genres listed)' else []\n",
    "            self.genre_features[mid] = np.array([1.0 if g in genres else 0.0 for g in self.genre_list], dtype=np.float32)\n",
    "        print(f\"  Genres: {len(self.genre_list)}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def build_target_encoding(self, train_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Standard Bayesian target encoding - computed ONLY on training data.\n",
    "        No adaptive smoothing - fixed smoothing strength.\n",
    "        \"\"\"\n",
    "        print(f\"\\nBuilding TARGET ENCODING (train data only)...\")\n",
    "        print(f\"  Smoothing strength: {CONFIG.smoothing_strength}\")\n",
    "        \n",
    "        explicit = train_df[train_df['rating'].notna()]\n",
    "        self.global_mean = explicit['rating'].mean()\n",
    "        \n",
    "        # User stats with standard smoothing\n",
    "        user_stats = explicit.groupby('user_id')['rating'].agg(['mean', 'count'])\n",
    "        for uid, row in user_stats.iterrows():\n",
    "            n, avg = row['count'], row['mean']\n",
    "            # Standard Bayesian smoothing: (n * avg + m * global_mean) / (n + m)\n",
    "            smoothed = (n * avg + CONFIG.smoothing_strength * self.global_mean) / (n + CONFIG.smoothing_strength)\n",
    "            self.user_mean_rating[uid] = smoothed\n",
    "            self.user_rating_count[uid] = n\n",
    "            \n",
    "            thresholds = CONFIG.user_activity_thresholds\n",
    "            if n < thresholds[0]: self.user_activity_bucket[uid] = 0\n",
    "            elif n < thresholds[1]: self.user_activity_bucket[uid] = 1\n",
    "            elif n < thresholds[2]: self.user_activity_bucket[uid] = 2\n",
    "            else: self.user_activity_bucket[uid] = 3\n",
    "        \n",
    "        # Item stats with standard smoothing\n",
    "        item_stats = explicit.groupby('movie_id')['rating'].agg(['mean', 'count'])\n",
    "        for mid, row in item_stats.iterrows():\n",
    "            n, avg = row['count'], row['mean']\n",
    "            smoothed = (n * avg + CONFIG.smoothing_strength * self.global_mean) / (n + CONFIG.smoothing_strength)\n",
    "            self.item_mean_rating[mid] = smoothed\n",
    "            self.item_rating_count[mid] = n\n",
    "            \n",
    "            thresholds = CONFIG.item_popularity_thresholds\n",
    "            if n < thresholds[0]: self.item_popularity_bucket[mid] = 0\n",
    "            elif n < thresholds[1]: self.item_popularity_bucket[mid] = 1\n",
    "            elif n < thresholds[2]: self.item_popularity_bucket[mid] = 2\n",
    "            else: self.item_popularity_bucket[mid] = 3\n",
    "        \n",
    "        print(f\"  Global mean: {self.global_mean:.4f}\")\n",
    "        print(f\"  Users with ratings: {len(self.user_mean_rating):,}\")\n",
    "        print(f\"  Items with ratings: {len(self.item_mean_rating):,}\")\n",
    "\n",
    "\n",
    "FEATURES = FeatureStore()\n",
    "print(\"Feature store initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wide & Deep Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeepModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified architecture for better generalization.\n",
    "    - Shallower deep tower: [256, 128]\n",
    "    - Consistent high dropout: 0.4\n",
    "    - Removed complex continuous features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_genres, global_mean=3.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.global_mean = nn.Parameter(torch.tensor([global_mean]), requires_grad=False)\n",
    "        \n",
    "        # === BIASES ===\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.bias_dropout = nn.Dropout(CONFIG.bias_dropout)\n",
    "        \n",
    "        # === EMBEDDINGS ===\n",
    "        self.user_emb = nn.Embedding(n_users, CONFIG.embedding_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, CONFIG.embedding_dim)\n",
    "        self.emb_dropout = nn.Dropout(CONFIG.embedding_dropout)\n",
    "        \n",
    "        # === WIDE PART ===\n",
    "        # Features: genres(19) + year_bucket(6) + user_activity(4) + item_pop(4) = 33\n",
    "        wide_input_dim = n_genres + 6 + 4 + 4\n",
    "        self.wide_hidden = nn.Linear(wide_input_dim, CONFIG.wide_hidden_dim)\n",
    "        self.wide_bn = nn.BatchNorm1d(CONFIG.wide_hidden_dim)\n",
    "        self.wide_dropout = nn.Dropout(CONFIG.wide_dropout)\n",
    "        self.wide_output = nn.Linear(CONFIG.wide_hidden_dim, 1)\n",
    "        \n",
    "        # === DEEP PART (Simplified) ===\n",
    "        # Input: user_emb(32) + item_emb(32) + genres(19) + year_normalized(1) = 84\n",
    "        deep_input_dim = CONFIG.embedding_dim * 2 + n_genres + 1\n",
    "        \n",
    "        self.deep_layers = nn.ModuleList()\n",
    "        self.deep_bns = nn.ModuleList()\n",
    "        self.deep_dropouts = nn.ModuleList()\n",
    "        \n",
    "        prev_dim = deep_input_dim\n",
    "        for hidden_dim in CONFIG.deep_layers:\n",
    "            self.deep_layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.deep_bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "            self.deep_dropouts.append(nn.Dropout(CONFIG.deep_dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.deep_output = nn.Linear(CONFIG.deep_layers[-1], 1)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        nn.init.normal_(self.user_emb.weight, 0, 0.01)\n",
    "        nn.init.normal_(self.item_emb.weight, 0, 0.01)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.wide_hidden.weight)\n",
    "        nn.init.zeros_(self.wide_hidden.bias)\n",
    "        nn.init.xavier_uniform_(self.wide_output.weight)\n",
    "        nn.init.zeros_(self.wide_output.bias)\n",
    "        \n",
    "        for layer in self.deep_layers:\n",
    "            nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        nn.init.xavier_uniform_(self.deep_output.weight)\n",
    "        nn.init.zeros_(self.deep_output.bias)\n",
    "    \n",
    "    def forward(self, user_idx, item_idx, genre, wide_features, year_normalized):\n",
    "        # === BIASES ===\n",
    "        u_bias = self.user_bias(user_idx).squeeze(-1)\n",
    "        i_bias = self.item_bias(item_idx).squeeze(-1)\n",
    "        if self.training:\n",
    "            u_bias = self.bias_dropout(u_bias.unsqueeze(-1)).squeeze(-1) / (1 - CONFIG.bias_dropout)\n",
    "            i_bias = self.bias_dropout(i_bias.unsqueeze(-1)).squeeze(-1) / (1 - CONFIG.bias_dropout)\n",
    "        \n",
    "        # === WIDE ===\n",
    "        wide_h = self.wide_dropout(F.relu(self.wide_bn(self.wide_hidden(wide_features))))\n",
    "        wide_out = self.wide_output(wide_h).squeeze(-1)\n",
    "        \n",
    "        # === EMBEDDINGS ===\n",
    "        u_emb = self.emb_dropout(self.user_emb(user_idx))\n",
    "        i_emb = self.emb_dropout(self.item_emb(item_idx))\n",
    "        \n",
    "        # === DEEP ===\n",
    "        deep_in = torch.cat([u_emb, i_emb, genre, year_normalized], dim=1)\n",
    "        \n",
    "        x = deep_in\n",
    "        for layer, bn, dropout in zip(self.deep_layers, self.deep_bns, self.deep_dropouts):\n",
    "            x = dropout(F.relu(bn(layer(x))))\n",
    "        \n",
    "        deep_out = self.deep_output(x).squeeze(-1)\n",
    "        \n",
    "        # === FINAL ===\n",
    "        return self.global_mean + u_bias + i_bias + wide_out + deep_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(user_ids, movie_ids):\n",
    "    \"\"\"Simplified feature preparation - no complex statistics.\"\"\"\n",
    "    n = len(user_ids)\n",
    "    n_genres = len(FEATURES.genre_list)\n",
    "    \n",
    "    # Genre features\n",
    "    genre = np.zeros((n, n_genres), dtype=np.float32)\n",
    "    for i, mid in enumerate(movie_ids):\n",
    "        if mid in FEATURES.genre_features:\n",
    "            genre[i] = FEATURES.genre_features[mid]\n",
    "    \n",
    "    # Wide features: genres + year_bucket + user_activity + item_pop\n",
    "    wide_dim = n_genres + 6 + 4 + 4\n",
    "    wide_features = np.zeros((n, wide_dim), dtype=np.float32)\n",
    "    \n",
    "    for i, (uid, mid) in enumerate(zip(user_ids, movie_ids)):\n",
    "        offset = 0\n",
    "        \n",
    "        # Genres\n",
    "        if mid in FEATURES.genre_features:\n",
    "            wide_features[i, :n_genres] = FEATURES.genre_features[mid]\n",
    "        offset += n_genres\n",
    "        \n",
    "        # Year bucket (one-hot, 6 buckets)\n",
    "        year_bucket = FEATURES.movie_year_bucket.get(mid, 3)\n",
    "        wide_features[i, offset + year_bucket] = 1.0\n",
    "        offset += 6\n",
    "        \n",
    "        # User activity bucket (one-hot, 4 buckets)\n",
    "        activity = FEATURES.user_activity_bucket.get(uid, 1)\n",
    "        wide_features[i, offset + activity] = 1.0\n",
    "        offset += 4\n",
    "        \n",
    "        # Item popularity bucket (one-hot, 4 buckets)\n",
    "        pop = FEATURES.item_popularity_bucket.get(mid, 1)\n",
    "        wide_features[i, offset + pop] = 1.0\n",
    "    \n",
    "    # Year normalized (single continuous feature for deep tower)\n",
    "    year_normalized = np.zeros((n, 1), dtype=np.float32)\n",
    "    for i, mid in enumerate(movie_ids):\n",
    "        year = FEATURES.movie_years.get(mid)\n",
    "        if year:\n",
    "            year_normalized[i, 0] = (year - 1990) / 30.0  # Normalize to roughly [-1, 1]\n",
    "    \n",
    "    return (torch.from_numpy(genre),\n",
    "            torch.from_numpy(wide_features),\n",
    "            torch.from_numpy(year_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df, val_df, seed=42):\n",
    "    \"\"\"\n",
    "    Training with high regularization and temporal validation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING WIDE & DEEP (seed={seed})\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Config: emb={CONFIG.embedding_dim}, layers={CONFIG.deep_layers}\")\n",
    "    print(f\"        dropout={CONFIG.deep_dropout}, weight_decay={CONFIG.weight_decay}\")\n",
    "    print(f\"        lr={CONFIG.lr}, patience={CONFIG.patience}\")\n",
    "    \n",
    "    set_all_seeds(seed)\n",
    "    \n",
    "    global_mean = float(train_df['rating'].mean())\n",
    "    n_users, n_items = FEATURES.n_users, FEATURES.n_items\n",
    "    n_genres = len(FEATURES.genre_list)\n",
    "    \n",
    "    # Prepare features\n",
    "    print(\"\\nPreparing features...\")\n",
    "    train_user_ids = train_df['user_id'].values\n",
    "    train_movie_ids = train_df['movie_id'].values\n",
    "    train_user_idx = np.array([FEATURES.user_id_to_idx.get(u, 0) for u in train_user_ids], dtype=np.int64)\n",
    "    train_item_idx = np.array([FEATURES.item_id_to_idx.get(m, 0) for m in train_movie_ids], dtype=np.int64)\n",
    "    train_ratings = train_df['rating'].values.astype(np.float32)\n",
    "    train_genre, train_wide, train_year = prepare_features(train_user_ids, train_movie_ids)\n",
    "    \n",
    "    val_user_ids = val_df['user_id'].values\n",
    "    val_movie_ids = val_df['movie_id'].values\n",
    "    val_user_idx = np.array([FEATURES.user_id_to_idx.get(u, 0) for u in val_user_ids], dtype=np.int64)\n",
    "    val_item_idx = np.array([FEATURES.item_id_to_idx.get(m, 0) for m in val_movie_ids], dtype=np.int64)\n",
    "    val_ratings = val_df['rating'].values.astype(np.float32)\n",
    "    val_genre, val_wide, val_year = prepare_features(val_user_ids, val_movie_ids)\n",
    "    \n",
    "    print(f\"Train: {len(train_ratings):,}, Val: {len(val_ratings):,}\")\n",
    "    \n",
    "    # Model\n",
    "    model = WideDeepModel(n_users, n_items, n_genres, global_mean=global_mean).to(DEVICE)\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model parameters: {n_params:,}\")\n",
    "    \n",
    "    # Optimizer with high weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.lr, weight_decay=CONFIG.weight_decay)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=CONFIG.scheduler_factor, \n",
    "        patience=CONFIG.scheduler_patience, min_lr=CONFIG.min_lr\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    train_user_t = torch.from_numpy(train_user_idx).to(DEVICE)\n",
    "    train_item_t = torch.from_numpy(train_item_idx).to(DEVICE)\n",
    "    train_rating_t = torch.from_numpy(train_ratings).to(DEVICE)\n",
    "    train_genre = train_genre.to(DEVICE)\n",
    "    train_wide = train_wide.to(DEVICE)\n",
    "    train_year = train_year.to(DEVICE)\n",
    "    \n",
    "    val_user_t = torch.from_numpy(val_user_idx).to(DEVICE)\n",
    "    val_item_t = torch.from_numpy(val_item_idx).to(DEVICE)\n",
    "    val_rating_t = torch.from_numpy(val_ratings).to(DEVICE)\n",
    "    val_genre = val_genre.to(DEVICE)\n",
    "    val_wide = val_wide.to(DEVICE)\n",
    "    val_year = val_year.to(DEVICE)\n",
    "    \n",
    "    # Training state\n",
    "    best_val_rmse = float('inf')\n",
    "    patience_cnt = 0\n",
    "    best_state = None\n",
    "    best_epoch = 0\n",
    "    \n",
    "    n_train = len(train_ratings)\n",
    "    n_batches = (n_train + CONFIG.batch_size - 1) // CONFIG.batch_size\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    for epoch in range(CONFIG.n_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Learning rate warmup\n",
    "        if epoch < CONFIG.warmup_epochs:\n",
    "            warmup_factor = (epoch + 1) / CONFIG.warmup_epochs\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg['lr'] = CONFIG.lr * warmup_factor\n",
    "        \n",
    "        perm = torch.randperm(n_train, device=DEVICE)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            s, e = b * CONFIG.batch_size, min((b + 1) * CONFIG.batch_size, n_train)\n",
    "            idx = perm[s:e]\n",
    "            \n",
    "            pred = model(train_user_t[idx], train_item_t[idx],\n",
    "                        train_genre[idx], train_wide[idx], train_year[idx])\n",
    "            loss = F.mse_loss(pred, train_rating_t[idx])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CONFIG.grad_clip)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * (e - s)\n",
    "        \n",
    "        train_rmse = np.sqrt(epoch_loss / n_train)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(val_user_t, val_item_t, val_genre, val_wide, val_year)\n",
    "            val_rmse = np.sqrt(F.mse_loss(val_pred, val_rating_t).item())\n",
    "        \n",
    "        # Step scheduler after warmup\n",
    "        if epoch >= CONFIG.warmup_epochs:\n",
    "            scheduler.step(val_rmse)\n",
    "        \n",
    "        gap = train_rmse - val_rmse\n",
    "        status = \"OK\" if gap > -0.05 else \"WARNING\" if gap > -0.1 else \"OVERFIT!\"\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"  Epoch {epoch+1:2d}: Train={train_rmse:.4f}, Val={val_rmse:.4f}, Gap={gap:+.4f} [{status}], LR={current_lr:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_epoch = epoch + 1\n",
    "            patience_cnt = 0\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= CONFIG.patience:\n",
    "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        if DEVICE.type == 'mps': torch.mps.empty_cache()\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "    \n",
    "    print(f\"\\n✓ Best Epoch: {best_epoch}, Val RMSE: {best_val_rmse:.4f}\")\n",
    "    gc.collect()\n",
    "    return model, best_val_rmse, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, user_ids, movie_ids, batch_size=8192):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    for start in range(0, len(user_ids), batch_size):\n",
    "        end = min(start + batch_size, len(user_ids))\n",
    "        batch_users, batch_movies = user_ids[start:end], movie_ids[start:end]\n",
    "        \n",
    "        user_idx = np.array([FEATURES.user_id_to_idx.get(u, 0) for u in batch_users], dtype=np.int64)\n",
    "        item_idx = np.array([FEATURES.item_id_to_idx.get(m, 0) for m in batch_movies], dtype=np.int64)\n",
    "        genre, wide, year = prepare_features(batch_users, batch_movies)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(torch.from_numpy(user_idx).to(DEVICE),\n",
    "                         torch.from_numpy(item_idx).to(DEVICE),\n",
    "                         genre.to(DEVICE), wide.to(DEVICE), year.to(DEVICE))\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        if DEVICE.type == 'mps': torch.mps.empty_cache()\n",
    "    \n",
    "    return np.concatenate(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WIDE & DEEP - SIMPLIFIED ARCHITECTURE\n",
      "======================================================================\n",
      "\n",
      "Loading data...\n",
      "Explicit ratings: 7,303,350\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WIDE & DEEP - SIMPLIFIED ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading data...\")\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "movies_df = pd.read_csv(f\"{DATA_DIR}/movies.csv\")\n",
    "submission_df = pd.read_csv(f\"{DATA_DIR}/ratings_submission.csv\")\n",
    "\n",
    "split_ids = submission_df['id'].str.split('_', expand=True)\n",
    "submission_df['user_id'] = split_ids[0].astype('int32')\n",
    "submission_df['movie_id'] = split_ids[1].astype('int32')\n",
    "\n",
    "train_explicit = train_df[train_df['rating'].notna()].copy()\n",
    "print(f\"Explicit ratings: {len(train_explicit):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BUILDING FEATURE STORE (Temporal Generalization)\n",
      "============================================================\n",
      "[1/3] Building ID mappings...\n",
      "  Users: 100,000, Items: 2,000\n",
      "[2/3] Extracting movie metadata...\n",
      "[3/3] Building genre features...\n",
      "  Genres: 19\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build basic features (ID mappings, genres, years)\n",
    "FEATURES.build_basic(train_df, submission_df, movies_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STRATIFIED USER SPLIT (25% validation)\n",
      "  Larger validation for better test RMSE estimate\n",
      "============================================================\n",
      "  Train: 5,514,044 ratings\n",
      "  Val: 1,789,306 ratings\n",
      "\n",
      "Building TARGET ENCODING (train data only)...\n",
      "  Smoothing strength: 80.0\n",
      "  Global mean: 3.6077\n",
      "  Users with ratings: 100,000\n",
      "  Items with ratings: 2,000\n"
     ]
    }
   ],
   "source": [
    "# === STRATIFIED USER SPLIT (25% val) ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRATIFIED USER SPLIT (25% validation)\")\n",
    "print(\"  Larger validation for better test RMSE estimate\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_split, val_split = stratified_rating_split(train_explicit, val_ratio=CONFIG.val_ratio)\n",
    "print(f\"  Train: {len(train_split):,} ratings\")\n",
    "print(f\"  Val: {len(val_split):,} ratings\")\n",
    "\n",
    "# Build target encoding on full data (users appear in both train/val)\n",
    "FEATURES.build_target_encoding(train_explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING ENSEMBLE (5 models)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=42)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 5,514,044, Val: 1,789,306\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9720, Val=0.8493, Gap=+0.1227 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8494, Val=0.8332, Gap=+0.0162 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8278, Val=0.8220, Gap=+0.0058 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8131, Val=0.8132, Gap=-0.0001 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.7998, Val=0.8075, Gap=-0.0077 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7893, Val=0.8036, Gap=-0.0143 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7808, Val=0.8007, Gap=-0.0199 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7738, Val=0.7984, Gap=-0.0245 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7679, Val=0.7966, Gap=-0.0287 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7631, Val=0.7960, Gap=-0.0330 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7587, Val=0.7951, Gap=-0.0363 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7547, Val=0.7945, Gap=-0.0397 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7512, Val=0.7942, Gap=-0.0431 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7480, Val=0.7937, Gap=-0.0458 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7450, Val=0.7930, Gap=-0.0480 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7423, Val=0.7931, Gap=-0.0508 [WARNING], LR=0.000500\n",
      "  Epoch 17: Train=0.7399, Val=0.7928, Gap=-0.0529 [WARNING], LR=0.000500\n",
      "  Epoch 18: Train=0.7375, Val=0.7930, Gap=-0.0555 [WARNING], LR=0.000500\n",
      "  Epoch 19: Train=0.7355, Val=0.7933, Gap=-0.0578 [WARNING], LR=0.000500\n",
      "  Epoch 20: Train=0.7335, Val=0.7925, Gap=-0.0590 [WARNING], LR=0.000500\n",
      "  Epoch 21: Train=0.7316, Val=0.7924, Gap=-0.0608 [WARNING], LR=0.000500\n",
      "  Epoch 22: Train=0.7299, Val=0.7932, Gap=-0.0633 [WARNING], LR=0.000500\n",
      "  Epoch 23: Train=0.7283, Val=0.7930, Gap=-0.0647 [WARNING], LR=0.000500\n",
      "  Epoch 24: Train=0.7271, Val=0.7923, Gap=-0.0653 [WARNING], LR=0.000500\n",
      "  Epoch 25: Train=0.7254, Val=0.7932, Gap=-0.0678 [WARNING], LR=0.000500\n",
      "\n",
      "✓ Best Epoch: 24, Val RMSE: 0.7923\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=123)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 5,514,044, Val: 1,789,306\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9968, Val=0.8490, Gap=+0.1478 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8487, Val=0.8322, Gap=+0.0165 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8268, Val=0.8221, Gap=+0.0047 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8127, Val=0.8137, Gap=-0.0010 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.7997, Val=0.8079, Gap=-0.0082 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7891, Val=0.8035, Gap=-0.0144 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7802, Val=0.8003, Gap=-0.0201 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7732, Val=0.7980, Gap=-0.0248 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7669, Val=0.7965, Gap=-0.0296 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7617, Val=0.7953, Gap=-0.0337 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7573, Val=0.7948, Gap=-0.0376 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7532, Val=0.7941, Gap=-0.0409 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7499, Val=0.7932, Gap=-0.0433 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7467, Val=0.7938, Gap=-0.0471 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7442, Val=0.7931, Gap=-0.0490 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7413, Val=0.7937, Gap=-0.0523 [WARNING], LR=0.000500\n",
      "  Epoch 17: Train=0.7390, Val=0.7935, Gap=-0.0545 [WARNING], LR=0.000500\n",
      "  Epoch 18: Train=0.7368, Val=0.7928, Gap=-0.0560 [WARNING], LR=0.000500\n",
      "  Epoch 19: Train=0.7346, Val=0.7932, Gap=-0.0586 [WARNING], LR=0.000500\n",
      "  Epoch 20: Train=0.7327, Val=0.7930, Gap=-0.0602 [WARNING], LR=0.000500\n",
      "  Epoch 21: Train=0.7309, Val=0.7928, Gap=-0.0620 [WARNING], LR=0.000500\n",
      "  Epoch 22: Train=0.7292, Val=0.7934, Gap=-0.0641 [WARNING], LR=0.000250\n",
      "  Epoch 23: Train=0.7198, Val=0.7933, Gap=-0.0735 [WARNING], LR=0.000250\n",
      "  Epoch 24: Train=0.7182, Val=0.7934, Gap=-0.0752 [WARNING], LR=0.000250\n",
      "  Epoch 25: Train=0.7168, Val=0.7931, Gap=-0.0763 [WARNING], LR=0.000250\n",
      "\n",
      "✓ Best Epoch: 21, Val RMSE: 0.7928\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=456)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 5,514,044, Val: 1,789,306\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9810, Val=0.8495, Gap=+0.1315 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8496, Val=0.8321, Gap=+0.0174 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8273, Val=0.8213, Gap=+0.0060 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8127, Val=0.8145, Gap=-0.0018 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.8005, Val=0.8085, Gap=-0.0080 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7891, Val=0.8025, Gap=-0.0135 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7798, Val=0.7996, Gap=-0.0198 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7724, Val=0.7971, Gap=-0.0247 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7666, Val=0.7957, Gap=-0.0291 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7617, Val=0.7948, Gap=-0.0331 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7573, Val=0.7944, Gap=-0.0371 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7533, Val=0.7939, Gap=-0.0406 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7499, Val=0.7933, Gap=-0.0434 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7465, Val=0.7931, Gap=-0.0466 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7435, Val=0.7928, Gap=-0.0493 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7407, Val=0.7927, Gap=-0.0520 [WARNING], LR=0.000500\n",
      "  Epoch 17: Train=0.7384, Val=0.7926, Gap=-0.0542 [WARNING], LR=0.000500\n",
      "  Epoch 18: Train=0.7360, Val=0.7928, Gap=-0.0568 [WARNING], LR=0.000500\n",
      "  Epoch 19: Train=0.7341, Val=0.7926, Gap=-0.0585 [WARNING], LR=0.000500\n",
      "  Epoch 20: Train=0.7319, Val=0.7931, Gap=-0.0612 [WARNING], LR=0.000500\n",
      "  Epoch 21: Train=0.7303, Val=0.7923, Gap=-0.0620 [WARNING], LR=0.000500\n",
      "  Epoch 22: Train=0.7287, Val=0.7924, Gap=-0.0637 [WARNING], LR=0.000500\n",
      "  Epoch 23: Train=0.7272, Val=0.7923, Gap=-0.0651 [WARNING], LR=0.000500\n",
      "  Epoch 24: Train=0.7256, Val=0.7925, Gap=-0.0669 [WARNING], LR=0.000500\n",
      "  Epoch 25: Train=0.7241, Val=0.7921, Gap=-0.0680 [WARNING], LR=0.000500\n",
      "\n",
      "✓ Best Epoch: 25, Val RMSE: 0.7921\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=789)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 5,514,044, Val: 1,789,306\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9794, Val=0.8499, Gap=+0.1295 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8489, Val=0.8324, Gap=+0.0165 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8283, Val=0.8230, Gap=+0.0053 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8149, Val=0.8158, Gap=-0.0009 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.8027, Val=0.8101, Gap=-0.0074 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7919, Val=0.8053, Gap=-0.0134 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7825, Val=0.8017, Gap=-0.0192 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7743, Val=0.7992, Gap=-0.0249 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7676, Val=0.7974, Gap=-0.0299 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7619, Val=0.7962, Gap=-0.0343 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7570, Val=0.7955, Gap=-0.0385 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7532, Val=0.7944, Gap=-0.0411 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7496, Val=0.7948, Gap=-0.0452 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7465, Val=0.7941, Gap=-0.0475 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7437, Val=0.7937, Gap=-0.0500 [WARNING], LR=0.000500\n",
      "  Epoch 16: Train=0.7413, Val=0.7934, Gap=-0.0521 [WARNING], LR=0.000500\n",
      "  Epoch 17: Train=0.7388, Val=0.7937, Gap=-0.0549 [WARNING], LR=0.000500\n",
      "  Epoch 18: Train=0.7367, Val=0.7936, Gap=-0.0569 [WARNING], LR=0.000500\n",
      "  Epoch 19: Train=0.7347, Val=0.7940, Gap=-0.0594 [WARNING], LR=0.000500\n",
      "  Epoch 20: Train=0.7327, Val=0.7937, Gap=-0.0610 [WARNING], LR=0.000250\n",
      "  Epoch 21: Train=0.7232, Val=0.7933, Gap=-0.0700 [WARNING], LR=0.000250\n",
      "  Epoch 22: Train=0.7212, Val=0.7938, Gap=-0.0726 [WARNING], LR=0.000250\n",
      "  Epoch 23: Train=0.7197, Val=0.7939, Gap=-0.0742 [WARNING], LR=0.000250\n",
      "  Epoch 24: Train=0.7184, Val=0.7939, Gap=-0.0754 [WARNING], LR=0.000250\n",
      "  Epoch 25: Train=0.7174, Val=0.7941, Gap=-0.0767 [WARNING], LR=0.000125\n",
      "\n",
      "✓ Best Epoch: 21, Val RMSE: 0.7933\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=2025)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 5,514,044, Val: 1,789,306\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9819, Val=0.8483, Gap=+0.1336 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8474, Val=0.8312, Gap=+0.0162 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8260, Val=0.8202, Gap=+0.0058 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8108, Val=0.8128, Gap=-0.0020 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.7988, Val=0.8083, Gap=-0.0095 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7889, Val=0.8032, Gap=-0.0143 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7805, Val=0.8000, Gap=-0.0195 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7734, Val=0.7979, Gap=-0.0245 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7676, Val=0.7969, Gap=-0.0293 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7626, Val=0.7958, Gap=-0.0332 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7583, Val=0.7948, Gap=-0.0366 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7544, Val=0.7945, Gap=-0.0402 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7508, Val=0.7940, Gap=-0.0432 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7477, Val=0.7937, Gap=-0.0460 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7446, Val=0.7936, Gap=-0.0490 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7423, Val=0.7928, Gap=-0.0506 [WARNING], LR=0.000500\n",
      "  Epoch 17: Train=0.7397, Val=0.7932, Gap=-0.0535 [WARNING], LR=0.000500\n",
      "  Epoch 18: Train=0.7374, Val=0.7928, Gap=-0.0554 [WARNING], LR=0.000500\n",
      "  Epoch 19: Train=0.7353, Val=0.7928, Gap=-0.0575 [WARNING], LR=0.000500\n",
      "  Epoch 20: Train=0.7331, Val=0.7925, Gap=-0.0594 [WARNING], LR=0.000500\n",
      "  Epoch 21: Train=0.7315, Val=0.7934, Gap=-0.0619 [WARNING], LR=0.000500\n",
      "  Epoch 22: Train=0.7297, Val=0.7924, Gap=-0.0627 [WARNING], LR=0.000500\n",
      "  Epoch 23: Train=0.7283, Val=0.7920, Gap=-0.0637 [WARNING], LR=0.000500\n",
      "  Epoch 24: Train=0.7267, Val=0.7928, Gap=-0.0661 [WARNING], LR=0.000500\n",
      "  Epoch 25: Train=0.7251, Val=0.7924, Gap=-0.0674 [WARNING], LR=0.000500\n",
      "\n",
      "✓ Best Epoch: 23, Val RMSE: 0.7920\n",
      "\n",
      "--- Per-Model Summary ---\n",
      "  Model 1 (seed=42): Val RMSE=0.7923, Best Epoch=24\n",
      "  Model 2 (seed=123): Val RMSE=0.7928, Best Epoch=21\n",
      "  Model 3 (seed=456): Val RMSE=0.7921, Best Epoch=25\n",
      "  Model 4 (seed=789): Val RMSE=0.7933, Best Epoch=21\n",
      "  Model 5 (seed=2025): Val RMSE=0.7920, Best Epoch=23\n",
      "  Average Val RMSE: 0.7925\n",
      "  Std Val RMSE: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# === ENSEMBLE TRAINING ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "n_seeds = min(CONFIG.n_seeds_to_use, len(CONFIG.seeds))\n",
    "seeds_to_use = CONFIG.seeds[:n_seeds]\n",
    "print(f\"TRAINING ENSEMBLE ({n_seeds} models)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = []\n",
    "val_rmses = []\n",
    "best_epochs = []\n",
    "\n",
    "for seed in seeds_to_use:\n",
    "    model, val_rmse, best_epoch = train_model(train_split, val_split, seed=seed)\n",
    "    models.append(model)\n",
    "    val_rmses.append(val_rmse)\n",
    "    best_epochs.append(best_epoch)\n",
    "    gc.collect()\n",
    "    if DEVICE.type == 'mps': torch.mps.empty_cache()\n",
    "\n",
    "print(f\"\\n--- Per-Model Summary ---\")\n",
    "for i, (seed, vr, be) in enumerate(zip(seeds_to_use, val_rmses, best_epochs)):\n",
    "    print(f\"  Model {i+1} (seed={seed}): Val RMSE={vr:.4f}, Best Epoch={be}\")\n",
    "print(f\"  Average Val RMSE: {np.mean(val_rmses):.4f}\")\n",
    "print(f\"  Std Val RMSE: {np.std(val_rmses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENSEMBLE EVALUATION (Temporal Validation)\n",
      "============================================================\n",
      "\n",
      "  ensemble_val_rmse (temporal): 0.7846\n",
      "  prediction_std_mean: 0.0950\n",
      "  prediction_std_95th_percentile: 0.2084\n",
      "  val_prediction_mean: 3.6043\n"
     ]
    }
   ],
   "source": [
    "# === ENSEMBLE EVALUATION ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE EVALUATION (Temporal Validation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "val_preds_list = []\n",
    "for model in models:\n",
    "    preds = predict_batch(model, val_split['user_id'].values, val_split['movie_id'].values)\n",
    "    val_preds_list.append(preds)\n",
    "\n",
    "val_preds_array = np.array(val_preds_list)\n",
    "ensemble_val_preds = val_preds_array.mean(axis=0)\n",
    "val_targets = val_split['rating'].values\n",
    "ensemble_rmse = np.sqrt(np.mean((ensemble_val_preds - val_targets) ** 2))\n",
    "\n",
    "pred_std = val_preds_array.std(axis=0)\n",
    "\n",
    "print(f\"\\n  ensemble_val_rmse (temporal): {ensemble_rmse:.4f}\")\n",
    "print(f\"  prediction_std_mean: {pred_std.mean():.4f}\")\n",
    "print(f\"  prediction_std_95th_percentile: {np.percentile(pred_std, 95):.4f}\")\n",
    "print(f\"  val_prediction_mean: {ensemble_val_preds.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RETRAINING ENSEMBLE ON FULL DATA\n",
      "============================================================\n",
      "\n",
      "Building TARGET ENCODING (train data only)...\n",
      "  Smoothing strength: 80.0\n",
      "  Global mean: 3.6081\n",
      "  Users with ratings: 100,000\n",
      "  Items with ratings: 2,000\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=42)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 6,980,647, Val: 322,703\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9484, Val=0.8452, Gap=+0.1032 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8441, Val=0.8256, Gap=+0.0186 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8245, Val=0.8151, Gap=+0.0094 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8106, Val=0.8054, Gap=+0.0052 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.7976, Val=0.7984, Gap=-0.0009 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7871, Val=0.7936, Gap=-0.0065 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7795, Val=0.7898, Gap=-0.0103 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7739, Val=0.7880, Gap=-0.0141 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7694, Val=0.7870, Gap=-0.0176 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7651, Val=0.7853, Gap=-0.0202 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7617, Val=0.7839, Gap=-0.0222 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7587, Val=0.7837, Gap=-0.0250 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7557, Val=0.7827, Gap=-0.0271 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7531, Val=0.7822, Gap=-0.0291 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7505, Val=0.7814, Gap=-0.0308 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7483, Val=0.7813, Gap=-0.0330 [OK], LR=0.000500\n",
      "  Epoch 17: Train=0.7460, Val=0.7807, Gap=-0.0347 [OK], LR=0.000500\n",
      "  Epoch 18: Train=0.7441, Val=0.7801, Gap=-0.0359 [OK], LR=0.000500\n",
      "  Epoch 19: Train=0.7423, Val=0.7800, Gap=-0.0377 [OK], LR=0.000500\n",
      "  Epoch 20: Train=0.7405, Val=0.7800, Gap=-0.0394 [OK], LR=0.000500\n",
      "  Epoch 21: Train=0.7391, Val=0.7794, Gap=-0.0403 [OK], LR=0.000500\n",
      "  Epoch 22: Train=0.7376, Val=0.7793, Gap=-0.0417 [OK], LR=0.000500\n",
      "  Epoch 23: Train=0.7361, Val=0.7791, Gap=-0.0430 [OK], LR=0.000500\n",
      "  Epoch 24: Train=0.7348, Val=0.7791, Gap=-0.0443 [OK], LR=0.000500\n",
      "  Epoch 25: Train=0.7334, Val=0.7790, Gap=-0.0455 [OK], LR=0.000500\n",
      "\n",
      "✓ Best Epoch: 25, Val RMSE: 0.7790\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=123)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 6,980,647, Val: 322,703\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9698, Val=0.8439, Gap=+0.1259 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8435, Val=0.8248, Gap=+0.0186 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8229, Val=0.8127, Gap=+0.0101 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8083, Val=0.8042, Gap=+0.0041 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.7963, Val=0.7976, Gap=-0.0012 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7869, Val=0.7932, Gap=-0.0063 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7791, Val=0.7896, Gap=-0.0105 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7731, Val=0.7876, Gap=-0.0146 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7681, Val=0.7859, Gap=-0.0177 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7640, Val=0.7846, Gap=-0.0207 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7604, Val=0.7836, Gap=-0.0232 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7573, Val=0.7829, Gap=-0.0256 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7545, Val=0.7826, Gap=-0.0281 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7521, Val=0.7824, Gap=-0.0303 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7494, Val=0.7818, Gap=-0.0323 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7476, Val=0.7811, Gap=-0.0334 [OK], LR=0.000500\n",
      "  Epoch 17: Train=0.7457, Val=0.7809, Gap=-0.0352 [OK], LR=0.000500\n",
      "  Epoch 18: Train=0.7437, Val=0.7807, Gap=-0.0370 [OK], LR=0.000500\n",
      "  Epoch 19: Train=0.7420, Val=0.7800, Gap=-0.0380 [OK], LR=0.000500\n",
      "  Epoch 20: Train=0.7406, Val=0.7801, Gap=-0.0395 [OK], LR=0.000500\n",
      "  Epoch 21: Train=0.7390, Val=0.7802, Gap=-0.0412 [OK], LR=0.000500\n",
      "  Epoch 22: Train=0.7375, Val=0.7803, Gap=-0.0427 [OK], LR=0.000500\n",
      "  Epoch 23: Train=0.7364, Val=0.7796, Gap=-0.0432 [OK], LR=0.000500\n",
      "  Epoch 24: Train=0.7349, Val=0.7797, Gap=-0.0448 [OK], LR=0.000500\n",
      "  Epoch 25: Train=0.7338, Val=0.7789, Gap=-0.0451 [OK], LR=0.000500\n",
      "\n",
      "✓ Best Epoch: 25, Val RMSE: 0.7789\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=456)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 6,980,647, Val: 322,703\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9566, Val=0.8453, Gap=+0.1113 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8446, Val=0.8255, Gap=+0.0191 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8237, Val=0.8140, Gap=+0.0097 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8095, Val=0.8046, Gap=+0.0049 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.7968, Val=0.7978, Gap=-0.0010 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7869, Val=0.7929, Gap=-0.0060 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7791, Val=0.7891, Gap=-0.0099 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7728, Val=0.7869, Gap=-0.0141 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7682, Val=0.7855, Gap=-0.0172 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7640, Val=0.7842, Gap=-0.0202 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7604, Val=0.7839, Gap=-0.0235 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7571, Val=0.7826, Gap=-0.0255 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7543, Val=0.7828, Gap=-0.0285 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7516, Val=0.7821, Gap=-0.0305 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7492, Val=0.7813, Gap=-0.0321 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7470, Val=0.7814, Gap=-0.0344 [OK], LR=0.000500\n",
      "  Epoch 17: Train=0.7449, Val=0.7808, Gap=-0.0358 [OK], LR=0.000500\n",
      "  Epoch 18: Train=0.7430, Val=0.7800, Gap=-0.0370 [OK], LR=0.000500\n",
      "  Epoch 19: Train=0.7413, Val=0.7800, Gap=-0.0387 [OK], LR=0.000500\n",
      "  Epoch 20: Train=0.7394, Val=0.7794, Gap=-0.0400 [OK], LR=0.000500\n",
      "  Epoch 21: Train=0.7379, Val=0.7791, Gap=-0.0411 [OK], LR=0.000500\n",
      "  Epoch 22: Train=0.7365, Val=0.7785, Gap=-0.0419 [OK], LR=0.000500\n",
      "  Epoch 23: Train=0.7349, Val=0.7785, Gap=-0.0435 [OK], LR=0.000500\n",
      "  Epoch 24: Train=0.7339, Val=0.7786, Gap=-0.0447 [OK], LR=0.000500\n",
      "  Epoch 25: Train=0.7327, Val=0.7782, Gap=-0.0455 [OK], LR=0.000500\n",
      "\n",
      "✓ Best Epoch: 25, Val RMSE: 0.7782\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=789)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 6,980,647, Val: 322,703\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9549, Val=0.8442, Gap=+0.1107 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8442, Val=0.8263, Gap=+0.0179 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8247, Val=0.8156, Gap=+0.0091 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8113, Val=0.8059, Gap=+0.0054 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.7985, Val=0.7990, Gap=-0.0005 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7881, Val=0.7936, Gap=-0.0055 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7798, Val=0.7904, Gap=-0.0106 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7734, Val=0.7874, Gap=-0.0140 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7678, Val=0.7861, Gap=-0.0183 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7635, Val=0.7845, Gap=-0.0211 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7597, Val=0.7837, Gap=-0.0240 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7565, Val=0.7828, Gap=-0.0263 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7535, Val=0.7822, Gap=-0.0287 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7510, Val=0.7820, Gap=-0.0310 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7486, Val=0.7816, Gap=-0.0330 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7462, Val=0.7811, Gap=-0.0349 [OK], LR=0.000500\n",
      "  Epoch 17: Train=0.7445, Val=0.7805, Gap=-0.0360 [OK], LR=0.000500\n",
      "  Epoch 18: Train=0.7427, Val=0.7801, Gap=-0.0374 [OK], LR=0.000500\n",
      "  Epoch 19: Train=0.7411, Val=0.7800, Gap=-0.0389 [OK], LR=0.000500\n",
      "  Epoch 20: Train=0.7395, Val=0.7791, Gap=-0.0396 [OK], LR=0.000500\n",
      "  Epoch 21: Train=0.7381, Val=0.7796, Gap=-0.0416 [OK], LR=0.000500\n",
      "  Epoch 22: Train=0.7367, Val=0.7796, Gap=-0.0429 [OK], LR=0.000500\n",
      "  Epoch 23: Train=0.7353, Val=0.7790, Gap=-0.0437 [OK], LR=0.000500\n",
      "  Epoch 24: Train=0.7339, Val=0.7785, Gap=-0.0447 [OK], LR=0.000500\n",
      "  Epoch 25: Train=0.7326, Val=0.7784, Gap=-0.0458 [OK], LR=0.000500\n",
      "\n",
      "✓ Best Epoch: 25, Val RMSE: 0.7784\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=2025)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 6,980,647, Val: 322,703\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9564, Val=0.8442, Gap=+0.1122 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8429, Val=0.8242, Gap=+0.0187 [OK], LR=0.000500\n",
      "  Epoch  3: Train=0.8235, Val=0.8146, Gap=+0.0088 [OK], LR=0.000500\n",
      "  Epoch  4: Train=0.8100, Val=0.8045, Gap=+0.0056 [OK], LR=0.000500\n",
      "  Epoch  5: Train=0.7975, Val=0.7980, Gap=-0.0005 [OK], LR=0.000500\n",
      "  Epoch  6: Train=0.7878, Val=0.7935, Gap=-0.0057 [OK], LR=0.000500\n",
      "  Epoch  7: Train=0.7804, Val=0.7901, Gap=-0.0098 [OK], LR=0.000500\n",
      "  Epoch  8: Train=0.7738, Val=0.7881, Gap=-0.0143 [OK], LR=0.000500\n",
      "  Epoch  9: Train=0.7687, Val=0.7864, Gap=-0.0178 [OK], LR=0.000500\n",
      "  Epoch 10: Train=0.7642, Val=0.7856, Gap=-0.0214 [OK], LR=0.000500\n",
      "  Epoch 11: Train=0.7606, Val=0.7841, Gap=-0.0235 [OK], LR=0.000500\n",
      "  Epoch 12: Train=0.7573, Val=0.7835, Gap=-0.0262 [OK], LR=0.000500\n",
      "  Epoch 13: Train=0.7544, Val=0.7831, Gap=-0.0287 [OK], LR=0.000500\n",
      "  Epoch 14: Train=0.7517, Val=0.7819, Gap=-0.0302 [OK], LR=0.000500\n",
      "  Epoch 15: Train=0.7492, Val=0.7818, Gap=-0.0326 [OK], LR=0.000500\n",
      "  Epoch 16: Train=0.7468, Val=0.7808, Gap=-0.0339 [OK], LR=0.000500\n",
      "  Epoch 17: Train=0.7449, Val=0.7808, Gap=-0.0359 [OK], LR=0.000500\n",
      "  Epoch 18: Train=0.7428, Val=0.7803, Gap=-0.0376 [OK], LR=0.000500\n",
      "  Epoch 19: Train=0.7411, Val=0.7797, Gap=-0.0385 [OK], LR=0.000500\n",
      "  Epoch 20: Train=0.7394, Val=0.7797, Gap=-0.0403 [OK], LR=0.000500\n",
      "  Epoch 21: Train=0.7377, Val=0.7795, Gap=-0.0417 [OK], LR=0.000500\n",
      "  Epoch 22: Train=0.7364, Val=0.7791, Gap=-0.0427 [OK], LR=0.000500\n",
      "  Epoch 23: Train=0.7348, Val=0.7787, Gap=-0.0439 [OK], LR=0.000500\n",
      "  Epoch 24: Train=0.7337, Val=0.7786, Gap=-0.0449 [OK], LR=0.000500\n",
      "  Epoch 25: Train=0.7323, Val=0.7788, Gap=-0.0465 [OK], LR=0.000500\n",
      "\n",
      "✓ Best Epoch: 24, Val RMSE: 0.7786\n"
     ]
    }
   ],
   "source": [
    "# === RETRAIN ON FULL DATA ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRAINING ENSEMBLE ON FULL DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For final training, use stratified split with smaller val ratio\n",
    "final_train, final_val = stratified_rating_split(train_explicit, val_ratio=CONFIG.final_val_ratio)\n",
    "\n",
    "# Rebuild target encoding on final training data\n",
    "FEATURES.build_target_encoding(final_train)\n",
    "\n",
    "final_models = []\n",
    "for seed in seeds_to_use:\n",
    "    model, _, _ = train_model(final_train, final_val, seed=seed)\n",
    "    final_models.append(model)\n",
    "    gc.collect()\n",
    "    if DEVICE.type == 'mps': torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING ENSEMBLE SUBMISSION\n",
      "============================================================\n",
      "\n",
      "✓ Saved: submission_wide_deep.csv\n",
      "Predictions: 100,000\n",
      "Rating range: [0.53, 5.00]\n",
      "Rating mean (submission): 3.7024\n",
      "\n",
      "======================================================================\n",
      "EXPECTED TEST RMSE (25% val): ~0.7846\n",
      "  Shallower model [256,128], moderate regularization\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# === GENERATE SUBMISSION ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING ENSEMBLE SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get ensemble predictions\n",
    "sub_preds_list = []\n",
    "for model in final_models:\n",
    "    preds = predict_batch(model, submission_df['user_id'].values, submission_df['movie_id'].values)\n",
    "    sub_preds_list.append(preds)\n",
    "\n",
    "ensemble_sub_preds = np.mean(sub_preds_list, axis=0)\n",
    "\n",
    "# Clip to valid range\n",
    "ensemble_sub_preds = np.clip(ensemble_sub_preds, 0.5, 5.0)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': submission_df['id'],\n",
    "    'prediction': ensemble_sub_preds\n",
    "})\n",
    "\n",
    "output_file = 'submission_wide_deep.csv'\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved: {output_file}\")\n",
    "print(f\"Predictions: {len(submission):,}\")\n",
    "print(f\"Rating range: [{ensemble_sub_preds.min():.2f}, {ensemble_sub_preds.max():.2f}]\")\n",
    "print(f\"Rating mean (submission): {ensemble_sub_preds.mean():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"EXPECTED TEST RMSE (25% val): ~{ensemble_rmse:.4f}\")\n",
    "print(\"  Shallower model [256,128], moderate regularization\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Cold Start Strategy\n",
    "\n",
    "This section addresses how the Wide & Deep model handles the cold-start problem in movie recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Cold-Start Problem in Movie Recommender Systems\n",
    "\n",
    "### What is Cold Start?\n",
    "\n",
    "The **cold-start problem** occurs when a recommender system must make predictions for entities it has never seen during training:\n",
    "\n",
    "- **New Users (User Cold Start)**: Users who have no or very few ratings in the training data. The system lacks collaborative signals to understand their preferences.\n",
    "\n",
    "- **New Items (Item Cold Start)**: Movies that have never been rated or have very few ratings. The system lacks user feedback to learn the item's quality or appeal.\n",
    "\n",
    "### Why is Cold Start Challenging?\n",
    "\n",
    "In collaborative filtering approaches, the model learns **user embeddings** and **item embeddings**. For new users/items:\n",
    "- Embeddings are random/zero-initialized → no useful information\n",
    "- Predictions default to global averages\n",
    "- Poor personalization and recommendation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Proposed Approach for Handling Cold Start on New Items\n",
    "\n",
    "The model implements **five complementary cold-start mechanisms for new items**:\n",
    "\n",
    "### A. Item Cold-Start Validation Split\n",
    "\n",
    "**Location**: `item_coldstart_split()` function\n",
    "\n",
    "The function holds out entire movies (10-15%) to simulate real cold-start scenarios:\n",
    "- Validation movies are completely unseen during training\n",
    "- This tests how well the model handles new items using only content features\n",
    "- Prevents data leakage by ensuring no validation item appears in training\n",
    "\n",
    "### B. Heavy Bayesian Smoothing (m=80)\n",
    "\n",
    "**Location**: `Config` and `build_target_encoding()`\n",
    "\n",
    "Uses standard Bayesian smoothing with high strength (m=80):\n",
    "```\n",
    "smoothed = (n * avg + m * global_mean) / (n + m)\n",
    "```\n",
    "\n",
    "**Effect**: Items with few ratings are heavily pulled toward the global mean, preventing unreliable predictions for rare items.\n",
    "\n",
    "### C. Content-Based Genre Features\n",
    "\n",
    "**Location**: `prepare_features()` function\n",
    "\n",
    "The wide component uses genre features (19 genres) extracted from movie metadata:\n",
    "- Available for ALL items, even those with zero ratings\n",
    "- Enables content-based recommendations for cold items\n",
    "- Combined with year buckets for temporal information\n",
    "\n",
    "### D. Zero-Initialized Item Bias\n",
    "\n",
    "**Location**: `WideDeepModel._init_weights()`\n",
    "\n",
    "Item biases start at zero:\n",
    "- New items have neutral bias\n",
    "- Predictions gracefully fall back to global mean + content features\n",
    "- No extreme predictions for unseen items\n",
    "\n",
    "### E. Global Mean Baseline\n",
    "\n",
    "**Location**: `WideDeepModel.forward()`\n",
    "\n",
    "Final prediction structure:\n",
    "```\n",
    "prediction = global_mean + user_bias + item_bias + wide_out + deep_out\n",
    "```\n",
    "\n",
    "Even with zero item bias and unlearned item embeddings, predictions are centered on a reasonable baseline using user embeddings and content features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assessment Methodology\n",
    "\n",
    "### How We Assess Cold Start Effectiveness for New Items\n",
    "\n",
    "To evaluate performance under true item cold-start conditions, we simulate the arrival of new movies by holding out entire items during training.\n",
    "\n",
    "Procedure:\n",
    "1. Randomly hold out 10% of movies from the dataset.\n",
    "2. Train the recommender system only on the remaining movies.\n",
    "3. Evaluate predictions only on ratings of the held-out movies, which are completely unseen during training.\n",
    "4. Since these movies have no training ratings, the model must rely on: \n",
    "    - Content features (genres, release year) \n",
    "    - User representations\n",
    "    - Global baseline statistics\n",
    "This setup prevents information leakage and reflects a realistic new-item deployment scenario.\n",
    "\n",
    "### Comparison Against Baselines\n",
    "\n",
    "| Baseline | Description | Method |\n",
    "|----------|-------------|--------|\n",
    "| **Global Mean** | Predict average rating for all | `pred = global_mean` |\n",
    "| **User Mean** | Average rating per user | `pred = mean(user_ratings)` |\n",
    "| **Genre Mean** | Average rating per genre | `pred = mean(genre_ratings)` |\n",
    "\n",
    "The Genre Mean baseline is a simple content-based method that serves as a strong non-personalized cold-item baseline.\n",
    "\n",
    "### Evaluation Code Pattern\n",
    "\n",
    "```python\n",
    "# Split: hold out entire items (movies)\n",
    "train_df, val_df = item_coldstart_split(data, val_item_ratio=0.10)\n",
    "\n",
    "# Train model on non-cold items only\n",
    "model = train_wide_deep(train_df)\n",
    "\n",
    "# Baseline 1: Global mean\n",
    "global_mean = train_df['rating'].mean()\n",
    "baseline_global_rmse = rmse(val_df['rating'], global_mean)\n",
    "\n",
    "# Baseline 2: User mean\n",
    "user_means = train_df.groupby('user_id')['rating'].mean()\n",
    "baseline_user_rmse = rmse(val_df['rating'], val_df['user_id'].map(user_means))\n",
    "\n",
    "# Our model predictions on cold items\n",
    "our_preds = predict_batch(model, val_df['user_id'], val_df['movie_id'])\n",
    "our_rmse = rmse(val_df['rating'], our_preds)\n",
    "\n",
    "improvement = (baseline_rmse - our_rmse) / baseline_rmse * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation Details\n",
    "\n",
    "### Key Implementation Sections:\n",
    "\n",
    "#### 4.1 Configuration\n",
    "- Heavy smoothing strength: 80.0\n",
    "- User activity thresholds: (10, 40, 100)\n",
    "- Item popularity thresholds: (30, 150, 400)\n",
    "\n",
    "#### 4.2 Item Cold-Start Split Function\n",
    "```python\n",
    "def item_coldstart_split(df: pd.DataFrame, val_item_ratio: float = 0.10, random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split by ITEMS - hold out val_item_ratio% of movies entirely.\n",
    "    This simulates cold-start scenario for new items not seen during training.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[ITEM COLD-START SPLIT] Holding out {val_item_ratio*100:.0f}% of items...\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get all unique items\n",
    "    all_items = df['movie_id'].unique()\n",
    "    n_items = len(all_items)\n",
    "    n_val_items = int(n_items * val_item_ratio)\n",
    "    \n",
    "    # Shuffle and split items\n",
    "    np.random.shuffle(all_items)\n",
    "    val_items = set(all_items[:n_val_items])\n",
    "    train_items = set(all_items[n_val_items:])\n",
    "    \n",
    "    # Split dataframe by items\n",
    "    train_df = df[df['movie_id'].isin(train_items)].reset_index(drop=True)\n",
    "    val_df = df[df['movie_id'].isin(val_items)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  Train items: {len(train_items):,}, Train ratings: {len(train_df):,}\")\n",
    "    print(f\"  Val items (cold): {len(val_items):,}, Val ratings: {len(val_df):,}\")\n",
    "    print(f\"  Avg ratings per train item: {len(train_df)/len(train_items):.1f}\")\n",
    "    print(f\"  Avg ratings per val item: {len(val_df)/len(val_items):.1f}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "```\n",
    "\n",
    "**Purpose**: Holds out 10% of movies entirely to simulate new items arriving in the system.\n",
    "\n",
    "#### 4.3 Target Encoding with Leakage Prevention\n",
    "```python\n",
    "def build_target_encoding(self, train_df):\n",
    "    # Computed ONLY on training data\n",
    "    explicit = train_df[train_df['rating'].notna()]\n",
    "    self.global_mean = explicit['rating'].mean()\n",
    "    \n",
    "    # Item stats with standard smoothing\n",
    "    for mid, row in item_stats.iterrows():\n",
    "        n, avg = row['count'], row['mean']\n",
    "        smoothed = (n * avg + CONFIG.smoothing_strength * self.global_mean) / (n + CONFIG.smoothing_strength)\n",
    "        self.item_mean_rating[mid] = smoothed\n",
    "```\n",
    "\n",
    "**Purpose**: Ensures item embeddings/biases are only learned from training items. Cold items get no training signal.\n",
    "\n",
    "#### 4.4 Content Feature Extraction\n",
    "Content features are extracted from movie metadata and are available for all items, including cold ones:\n",
    "Genres: 19-dimensional multi-hot vector\n",
    "Release year: Discretized into 6 buckets\n",
    "```python\n",
    "def prepare_features(user_ids, movie_ids):\n",
    "    # Genre features (available for ALL movies)\n",
    "    for i, mid in enumerate(movie_ids):\n",
    "        if mid in FEATURES.genre_features:\n",
    "            genre[i] = FEATURES.genre_features[mid]\n",
    "    \n",
    "    # Year bucket (one-hot, 6 buckets)\n",
    "    year_bucket = FEATURES.movie_year_bucket.get(mid, 3)\n",
    "    wide_features[i, offset + year_bucket] = 1.0\n",
    "```\n",
    "\n",
    "**Purpose**: Extracts content features (genres, year) that are available even for cold items with zero training ratings.\n",
    "\n",
    "#### 4.5 Model Architecture\n",
    "- **Item biases initialized to zero** for graceful cold-start handling\n",
    "- **Wide part** uses content features (genres, year buckets)\n",
    "- **Deep part** uses embeddings + content features\n",
    "- **Final prediction** includes global mean baseline\n",
    "\n",
    "```python\n",
    "# Forward pass\n",
    "prediction = global_mean + user_bias + item_bias + wide_out + deep_out\n",
    "# For cold items: item_bias ≈ 0, item_emb is random\n",
    "# Model relies on: user_bias, wide_out (content), user_emb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstration of Effectiveness\n",
    "\n",
    "### Actual Results\n",
    "\n",
    "Based on item cold-start validation (movies unseen during training):\n",
    "\n",
    "| Method | Cold Item RMSE | Notes |\n",
    "|--------|----------------|-------|\n",
    "| **Global Mean (Baseline)** | 1.0162 | No personalization or content |\n",
    "| **User Mean (Baseline)** | 0.9199 | Uses user history only |\n",
    "| **Our Model (Single)** | 0.8914 | Uses content features + user embeddings |\n",
    "\n",
    "**Performance Improvements:**\n",
    "- **12.3% improvement** over Global Mean baseline\n",
    "- **3.1% improvement** over User Mean baseline\n",
    "\n",
    "### Why the Model Handles Cold Items Well\n",
    "\n",
    "1. **Content Features**: Genres (19-dimensional) and year information provide signal even for items with zero ratings.\n",
    "\n",
    "2. **User Embeddings**: Known users have learned preferences that transfer to new items through content similarity.\n",
    "\n",
    "3. **Zero-Initialized Item Bias**: New items start neutral, avoiding extreme predictions.\n",
    "\n",
    "4. **Wide Tower**: Directly processes genre and year features to make content-based predictions.\n",
    "\n",
    "5. **Hybrid Approach**: Combines collaborative filtering (user embeddings) with content-based features (genres, year).\n",
    "\n",
    "### Cold Start Behavior Summary\n",
    "\n",
    "| Scenario | User Bias | Item Bias | Wide (Content) | Deep | Prediction Quality |\n",
    "|----------|-----------|-----------|----------------|------|-----------------|\n",
    "| Both warm | Learned | Learned | Genre+Year | User+Item emb | Best |\n",
    "| Cold item | Learned | ~0 | Genre+Year | User emb | Good (content+user) |\n",
    "| Cold user | ~0 | Learned | Genre+Year | Item emb | Good (content+item) |\n",
    "| Both cold | ~0 | ~0 | Genre+Year | ~0 | Reasonable (content only) |\n",
    "\n",
    "**Key Insight**: For cold items, the model can leverage:\n",
    "- User embeddings (learned preferences)\n",
    "- Content features (genres, year)\n",
    "- Wide tower (processes content directly)\n",
    "\n",
    "This enables strong performance even on movies with no training ratings, achieving substantial improvements over naive baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The model handles **cold items (new movies)** through **five integrated mechanisms**:\n",
    "\n",
    "| Mechanism | Purpose | Location |\n",
    "|-----------|---------|----------|\n",
    "| ✅ Item Cold-Start Split | Validate on unseen movies | `item_coldstart_split()` |\n",
    "| ✅ Content Features (Genres+Year) | Signal for cold items | `prepare_features()` |\n",
    "| ✅ User Embeddings | Known users + content → good predictions | `WideDeepModel` |\n",
    "| ✅ Zero-Initialized Item Biases | Graceful degradation | `_init_weights()` |\n",
    "| ✅ Global Mean Baseline | Safe default prediction | `forward()` |\n",
    "\n",
    "**Key Advantage**: Even for movies with zero training ratings, the model can leverage:\n",
    "- 19-dimensional genre vectors\n",
    "- Year information (6 buckets + normalized)\n",
    "- Learned user preferences\n",
    "- Wide tower for direct content processing\n",
    "\n",
    "This enables **content-based recommendations** that significantly outperform naive baselines, making the system robust to new item introductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COLD ITEM DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "[ITEM COLD-START SPLIT] Holding out 10% of items...\n",
      "  Train items: 1,800, Train ratings: 6,573,412\n",
      "  Val items (cold): 200, Val ratings: 729,938\n",
      "  Avg ratings per train item: 3651.9\n",
      "  Avg ratings per val item: 3649.7\n",
      "\n",
      "Cold items: 200 movies never seen during training\n",
      "These items will test content-based features (genres, year)\n",
      "\n",
      "============================================================\n",
      "BUILDING FEATURE STORE (Temporal Generalization)\n",
      "============================================================\n",
      "[1/3] Building ID mappings...\n",
      "  Users: 100,000, Items: 2,000\n",
      "[2/3] Extracting movie metadata...\n",
      "[3/3] Building genre features...\n",
      "  Genres: 19\n",
      "============================================================\n",
      "\n",
      "Building TARGET ENCODING (train data only)...\n",
      "  Smoothing strength: 80.0\n",
      "  Global mean: 3.6070\n",
      "  Users with ratings: 100,000\n",
      "  Items with ratings: 1,800\n",
      "\n",
      "[Training model on non-cold items only]\n",
      "\n",
      "============================================================\n",
      "TRAINING WIDE & DEEP (seed=42)\n",
      "============================================================\n",
      "Config: emb=32, layers=(256, 128)\n",
      "        dropout=0.35, weight_decay=0.005\n",
      "        lr=0.0005, patience=6\n",
      "\n",
      "Preparing features...\n",
      "Train: 6,573,412, Val: 729,938\n",
      "Model parameters: 3,422,738\n",
      "\n",
      "Training...\n",
      "  Epoch  1: Train=0.9541, Val=0.9153, Gap=+0.0388 [OK], LR=0.000250\n",
      "  Epoch  2: Train=0.8451, Val=0.9116, Gap=-0.0665 [WARNING], LR=0.000500\n",
      "  Epoch  3: Train=0.8231, Val=0.9046, Gap=-0.0814 [WARNING], LR=0.000500\n",
      "  Epoch  4: Train=0.8093, Val=0.8987, Gap=-0.0895 [WARNING], LR=0.000500\n",
      "  Epoch  5: Train=0.7972, Val=0.8943, Gap=-0.0971 [WARNING], LR=0.000500\n",
      "  Epoch  6: Train=0.7876, Val=0.8955, Gap=-0.1079 [OVERFIT!], LR=0.000500\n",
      "  Epoch  7: Train=0.7798, Val=0.8914, Gap=-0.1115 [OVERFIT!], LR=0.000500\n",
      "  Epoch  8: Train=0.7739, Val=0.8937, Gap=-0.1198 [OVERFIT!], LR=0.000500\n",
      "  Epoch  9: Train=0.7689, Val=0.8927, Gap=-0.1238 [OVERFIT!], LR=0.000500\n",
      "  Epoch 10: Train=0.7646, Val=0.8932, Gap=-0.1286 [OVERFIT!], LR=0.000500\n",
      "  Epoch 11: Train=0.7610, Val=0.8928, Gap=-0.1318 [OVERFIT!], LR=0.000250\n",
      "  Epoch 12: Train=0.7498, Val=0.8944, Gap=-0.1446 [OVERFIT!], LR=0.000250\n",
      "  Epoch 13: Train=0.7470, Val=0.8948, Gap=-0.1477 [OVERFIT!], LR=0.000250\n",
      "  Early stopping at epoch 13\n",
      "\n",
      "✓ Best Epoch: 7, Val RMSE: 0.8914\n",
      "\n",
      "============================================================\n",
      "COLD ITEM RESULTS (Movies Never Seen During Training)\n",
      "============================================================\n",
      "  Baseline (Global Mean):  RMSE = 1.0162\n",
      "  Baseline (User Mean):    RMSE = 0.9199\n",
      "  Our Model (Content):     RMSE = 0.8914\n",
      "\n",
      "  Improvement over Global: 12.3%\n",
      "  Improvement over User:   3.1%\n",
      "============================================================\n",
      "\n",
      "✓ Model successfully handles cold items using:\n",
      "  - Genre features (19 genres)\n",
      "  - Year information\n",
      "  - User embeddings (users are known)\n",
      "  - Global mean baseline\n"
     ]
    }
   ],
   "source": [
    "## 6. Cold-Start Performance Demonstration (New Items)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLD ITEM DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data with item cold-start (hold out 10% of movies)\n",
    "train_cold, val_cold = item_coldstart_split(train_explicit, val_item_ratio=0.10, random_state=42)\n",
    "\n",
    "print(f\"\\nCold items: {val_cold['movie_id'].nunique()} movies never seen during training\")\n",
    "print(f\"These items will test content-based features (genres, year)\")\n",
    "\n",
    "# Build features ONLY on training items\n",
    "FEATURES_COLD = FeatureStore()\n",
    "FEATURES_COLD.build_basic(train_cold, val_cold, movies_df)\n",
    "FEATURES_COLD.build_target_encoding(train_cold)\n",
    "\n",
    "# Train a single model on non-cold items\n",
    "print(\"\\n[Training model on non-cold items only]\")\n",
    "set_all_seeds(42)\n",
    "original_features = FEATURES\n",
    "FEATURES = FEATURES_COLD  # Temporarily swap to use cold features\n",
    "model_cold, _, _ = train_model(train_cold, val_cold, seed=42)\n",
    "\n",
    "# === BASELINE 1: Global Mean ===\n",
    "baseline_global = train_cold['rating'].mean()\n",
    "baseline_global_preds = np.full(len(val_cold), baseline_global)\n",
    "baseline_global_rmse = np.sqrt(np.mean((val_cold['rating'] - baseline_global_preds) ** 2))\n",
    "\n",
    "# === BASELINE 2: User Mean ===\n",
    "user_means = train_cold.groupby('user_id')['rating'].mean().to_dict()\n",
    "baseline_user_preds = val_cold['user_id'].map(lambda u: user_means.get(u, baseline_global)).values\n",
    "baseline_user_rmse = np.sqrt(np.mean((val_cold['rating'] - baseline_user_preds) ** 2))\n",
    "\n",
    "# === OUR MODEL ===\n",
    "model_preds = predict_batch(model_cold, val_cold['user_id'].values, val_cold['movie_id'].values)\n",
    "model_rmse = np.sqrt(np.mean((val_cold['rating'] - model_preds) ** 2))\n",
    "\n",
    "# Restore original features\n",
    "FEATURES = original_features\n",
    "\n",
    "# === RESULTS ===\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COLD ITEM RESULTS (Movies Never Seen During Training)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Baseline (Global Mean):  RMSE = {baseline_global_rmse:.4f}\")\n",
    "print(f\"  Baseline (User Mean):    RMSE = {baseline_user_rmse:.4f}\")\n",
    "print(f\"  Our Model (Content):     RMSE = {model_rmse:.4f}\")\n",
    "print(f\"\\n  Improvement over Global: {(baseline_global_rmse - model_rmse)/baseline_global_rmse * 100:.1f}%\")\n",
    "print(f\"  Improvement over User:   {(baseline_user_rmse - model_rmse)/baseline_user_rmse * 100:.1f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✓ Model successfully handles cold items using:\")\n",
    "print(\"  - Genre features (19 genres)\")\n",
    "print(\"  - Year information\")\n",
    "print(\"  - User embeddings (users are known)\")\n",
    "print(\"  - Global mean baseline\")\n",
    "\n",
    "# Clean up\n",
    "del FEATURES_COLD, model_cold\n",
    "gc.collect()\n",
    "if DEVICE.type == 'mps': torch.mps.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation_project-UWoI-MKR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
