{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfac MovieLens Recommendation System - Google Colab Edition\n",
        "\n",
        "This notebook contains all recommender methods with **separate cells for each experiment**.\n",
        "\n",
        "## Features\n",
        "- **BaselineRecommender**: Global mean baseline\n",
        "- **SVDRecommender**: Truncated SVD with biases  \n",
        "- **FunkSVDRecommender**: SGD-based matrix factorization (PyTorch GPU)\n",
        "- **ALSRecommender**: Alternating Least Squares (PyTorch GPU)\n",
        "- **HybridRecommender**: CF + Content features\n",
        "- **CompetitionEnsemble**: Stacked ensemble with LightGBM\n",
        "\n",
        "## Instructions\n",
        "1. **Upload your data**: Upload `train.csv`, `movies.csv`, `tags.csv`, and `ratings_submission.csv`\n",
        "2. **Run cells 1-10** to load all code and data\n",
        "3. **Run cells 11-17** independently for each experiment\n",
        "4. **Cell 18** shows final results summary\n",
        "\n",
        "\u2705 **Resume-friendly**: If disconnected, just re-run the cell you were on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q pandas numpy scikit-learn scipy torch lightgbm\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional, Any\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD as SklearnTruncatedSVD\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Device selection for GPU acceleration\n",
        "def get_device() -> torch.device:\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "DEVICE = get_device()\n",
        "print(f\"\u2713 Using device: {DEVICE}\")\n",
        "\n",
        "# Check LightGBM\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LGBM_AVAILABLE = True\n",
        "    print(\"\u2713 LightGBM available\")\n",
        "except ImportError:\n",
        "    LGBM_AVAILABLE = False\n",
        "    from sklearn.ensemble import GradientBoostingRegressor\n",
        "    print(\"\u26a0 LightGBM not available, using sklearn GradientBoosting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload data files (Colab-specific)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Upload your data files (train.csv, movies.csv, tags.csv, ratings_submission.csv)...\")\n",
        "    uploaded = files.upload()\n",
        "    DATA_DIR = \".\"\n",
        "    print(f\"\u2713 Uploaded {len(uploaded)} files\")\n",
        "except ImportError:\n",
        "    DATA_DIR = \"recsys-runi-2026/\"\n",
        "    print(f\"Running locally. Data directory: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loader & Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    \"\"\"Loads and preprocesses competition data files.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir: str = \".\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.train_df = None\n",
        "        self.movies_df = None\n",
        "        self.tags_df = None\n",
        "        self.submission_template_df = None\n",
        "        \n",
        "    def load_train_data(self, explicit_only: bool = True) -> pd.DataFrame:\n",
        "        print(\"Loading training data...\")\n",
        "        self.train_df = pd.read_csv(\n",
        "            self.data_dir / \"train.csv\",\n",
        "            dtype={'user_id': 'int32', 'movie_id': 'int32', 'rating': 'float32'}\n",
        "        )\n",
        "        print(f\"Loaded {len(self.train_df):,} interactions\")\n",
        "        if explicit_only:\n",
        "            self.train_df = self.train_df.dropna(subset=['rating'])\n",
        "            print(f\"Filtered to {len(self.train_df):,} explicit ratings\")\n",
        "        return self.train_df\n",
        "    \n",
        "    def load_movies_metadata(self) -> pd.DataFrame:\n",
        "        print(\"Loading movies metadata...\")\n",
        "        self.movies_df = pd.read_csv(self.data_dir / \"movies.csv\", dtype={'movie_id': 'int32'})\n",
        "        print(f\"Loaded {len(self.movies_df):,} movies\")\n",
        "        return self.movies_df\n",
        "    \n",
        "    def load_tags(self) -> pd.DataFrame:\n",
        "        print(\"Loading tags...\")\n",
        "        self.tags_df = pd.read_csv(self.data_dir / \"tags.csv\", dtype={'user_id': 'int32', 'movie_id': 'int32'})\n",
        "        print(f\"Loaded {len(self.tags_df):,} tags\")\n",
        "        return self.tags_df\n",
        "    \n",
        "    def load_submission_template(self) -> pd.DataFrame:\n",
        "        print(\"Loading submission template...\")\n",
        "        self.submission_template_df = pd.read_csv(self.data_dir / \"ratings_submission.csv\")\n",
        "        split_ids = self.submission_template_df['id'].str.split('_', expand=True)\n",
        "        self.submission_template_df['user_id'] = split_ids[0].astype('int32')\n",
        "        self.submission_template_df['movie_id'] = split_ids[1].astype('int32')\n",
        "        print(f\"Loaded {len(self.submission_template_df):,} submission pairs\")\n",
        "        return self.submission_template_df\n",
        "    \n",
        "    def get_genre_features(self) -> Dict[int, np.ndarray]:\n",
        "        if self.movies_df is None:\n",
        "            self.load_movies_metadata()\n",
        "        all_genres = set()\n",
        "        for genres_str in self.movies_df['genres'].dropna():\n",
        "            all_genres.update(genres_str.split('|'))\n",
        "        all_genres = sorted(list(all_genres))\n",
        "        genre_features = {}\n",
        "        for _, row in self.movies_df.iterrows():\n",
        "            movie_id = row['movie_id']\n",
        "            genres = row['genres'].split('|') if pd.notna(row['genres']) else []\n",
        "            genre_vector = np.array([1 if g in genres else 0 for g in all_genres], dtype=np.float32)\n",
        "            genre_features[movie_id] = genre_vector\n",
        "        print(f\"Extracted genre features with {len(all_genres)} unique genres\")\n",
        "        return genre_features\n",
        "\n",
        "# Evaluation Functions\n",
        "def compute_weighted_rmse(y_true, y_pred, movie_ids, train_df):\n",
        "    movie_counts = train_df.groupby('movie_id').size().to_dict()\n",
        "    weights = np.array([1.0 / np.sqrt(movie_counts.get(mid, 1)) for mid in movie_ids])\n",
        "    squared_errors = (y_pred - y_true) ** 2\n",
        "    weighted_squared_errors = weights * squared_errors\n",
        "    return np.sqrt(weighted_squared_errors.sum() / weights.sum())\n",
        "\n",
        "def compute_rmse(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "def evaluate_recommender(recommender, test_df, train_df):\n",
        "    pairs = list(zip(test_df['user_id'], test_df['movie_id']))\n",
        "    y_pred = np.clip(recommender.predict_batch(pairs), 0.5, 5.0)\n",
        "    y_true = test_df['rating'].values\n",
        "    movie_ids = test_df['movie_id'].values\n",
        "    return {\n",
        "        'weighted_rmse': compute_weighted_rmse(y_true, y_pred, movie_ids, train_df),\n",
        "        'rmse': compute_rmse(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "def train_val_test_split(df, val_size=0.15, test_size=0.15, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    n = len(df)\n",
        "    indices = np.random.permutation(n)\n",
        "    test_end = int(n * test_size)\n",
        "    val_end = test_end + int(n * val_size)\n",
        "    test_df = df.iloc[indices[:test_end]].copy()\n",
        "    val_df = df.iloc[indices[test_end:val_end]].copy()\n",
        "    train_df = df.iloc[indices[val_end:]].copy()\n",
        "    print(f\"Split: Train={len(train_df):,}, Val={len(val_df):,}, Test={len(test_df):,}\")\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "print(\"\u2713 Data utilities defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Base Recommender Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseRecommender(ABC):\n",
        "    \"\"\"Abstract base class for all recommendation algorithms.\"\"\"\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        self.is_fitted = False\n",
        "        self.kwargs = kwargs\n",
        "    \n",
        "    @abstractmethod\n",
        "    def fit(self, train_df: pd.DataFrame) -> 'BaseRecommender':\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def predict(self, user_id: int, movie_id: int) -> float:\n",
        "        pass\n",
        "    \n",
        "    def predict_batch(self, pairs: List[Tuple[int, int]]) -> np.ndarray:\n",
        "        return np.array([self.predict(u, m) for u, m in pairs])\n",
        "    \n",
        "    def predict_dataframe(self, df: pd.DataFrame) -> np.ndarray:\n",
        "        pairs = list(zip(df['user_id'], df['movie_id']))\n",
        "        return self.predict_batch(pairs)\n",
        "\n",
        "class BaselineRecommender(BaseRecommender):\n",
        "    \"\"\"Simple baseline recommender that predicts global mean rating.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.global_mean = None\n",
        "    \n",
        "    def fit(self, train_df: pd.DataFrame) -> 'BaselineRecommender':\n",
        "        self.global_mean = train_df['rating'].mean()\n",
        "        self.is_fitted = True\n",
        "        print(f\"Baseline fitted with global mean = {self.global_mean:.4f}\")\n",
        "        return self\n",
        "    \n",
        "    def predict(self, user_id: int, movie_id: int) -> float:\n",
        "        return self.global_mean\n",
        "    \n",
        "    def predict_batch(self, pairs: List[Tuple[int, int]]) -> np.ndarray:\n",
        "        return np.full(len(pairs), self.global_mean, dtype=np.float32)\n",
        "\n",
        "print(\"\u2713 Base recommender classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. SVD Recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SVDRecommender(BaseRecommender):\n",
        "    \"\"\"Matrix Factorization using SVD with user and item biases.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_factors: int = 50, random_state: int = 42):\n",
        "        super().__init__()\n",
        "        self.n_factors = n_factors\n",
        "        self.random_state = random_state\n",
        "        self.global_mean = None\n",
        "        self.user_biases = {}\n",
        "        self.item_biases = {}\n",
        "        self.user_factors = None\n",
        "        self.item_factors = None\n",
        "        self.singular_values = None\n",
        "        self.user_id_to_idx = {}\n",
        "        self.item_id_to_idx = {}\n",
        "    \n",
        "    def fit(self, train_df: pd.DataFrame) -> 'SVDRecommender':\n",
        "        print(f\"Training SVD with {self.n_factors} factors...\")\n",
        "        self.global_mean = train_df['rating'].mean()\n",
        "        \n",
        "        unique_users = train_df['user_id'].unique()\n",
        "        unique_items = train_df['movie_id'].unique()\n",
        "        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
        "        self.item_id_to_idx = {iid: idx for idx, iid in enumerate(unique_items)}\n",
        "        n_users, n_items = len(unique_users), len(unique_items)\n",
        "        print(f\"Users: {n_users:,}, Items: {n_items:,}\")\n",
        "        \n",
        "        user_means = train_df.groupby('user_id')['rating'].mean()\n",
        "        self.user_biases = (user_means - self.global_mean).to_dict()\n",
        "        item_means = train_df.groupby('movie_id')['rating'].mean()\n",
        "        self.item_biases = (item_means - self.global_mean).to_dict()\n",
        "        \n",
        "        user_indices = train_df['user_id'].map(self.user_id_to_idx).values\n",
        "        item_indices = train_df['movie_id'].map(self.item_id_to_idx).values\n",
        "        ratings = train_df['rating'].values.copy()\n",
        "        \n",
        "        for i, (uid, iid) in enumerate(zip(train_df['user_id'], train_df['movie_id'])):\n",
        "            ratings[i] -= self.global_mean + self.user_biases.get(uid, 0) + self.item_biases.get(iid, 0)\n",
        "        \n",
        "        rating_matrix = csr_matrix((ratings, (user_indices, item_indices)), shape=(n_users, n_items))\n",
        "        k = min(self.n_factors, min(n_users, n_items) - 1)\n",
        "        U, s, Vt = svds(rating_matrix, k=k, random_state=self.random_state)\n",
        "        \n",
        "        self.user_factors = U\n",
        "        self.singular_values = s\n",
        "        self.item_factors = Vt\n",
        "        self.is_fitted = True\n",
        "        print(f\"\u2713 SVD training complete!\")\n",
        "        return self\n",
        "    \n",
        "    def predict(self, user_id: int, movie_id: int) -> float:\n",
        "        prediction = self.global_mean + self.user_biases.get(user_id, 0) + self.item_biases.get(movie_id, 0)\n",
        "        user_idx = self.user_id_to_idx.get(user_id)\n",
        "        item_idx = self.item_id_to_idx.get(movie_id)\n",
        "        if user_idx is not None and item_idx is not None:\n",
        "            prediction += np.dot(self.user_factors[user_idx] * self.singular_values, self.item_factors[:, item_idx])\n",
        "        return prediction\n",
        "    \n",
        "    def predict_batch(self, pairs) -> np.ndarray:\n",
        "        predictions = np.full(len(pairs), self.global_mean, dtype=np.float32)\n",
        "        for i, (user_id, movie_id) in enumerate(pairs):\n",
        "            predictions[i] += self.user_biases.get(user_id, 0) + self.item_biases.get(movie_id, 0)\n",
        "            user_idx = self.user_id_to_idx.get(user_id)\n",
        "            item_idx = self.item_id_to_idx.get(movie_id)\n",
        "            if user_idx is not None and item_idx is not None:\n",
        "                predictions[i] += np.dot(self.user_factors[user_idx] * self.singular_values, self.item_factors[:, item_idx])\n",
        "        return predictions\n",
        "\n",
        "print(\"\u2713 SVD recommender defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. FunkSVD Recommender (GPU Accelerated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FunkSVDModel(nn.Module):\n",
        "    \"\"\"PyTorch module for FunkSVD matrix factorization.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_users: int, n_items: int, n_factors: int, global_mean: float):\n",
        "        super().__init__()\n",
        "        self.global_mean = global_mean\n",
        "        \n",
        "        # Learnable parameters\n",
        "        self.user_biases = nn.Embedding(n_users, 1)\n",
        "        self.item_biases = nn.Embedding(n_items, 1)\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.item_factors = nn.Embedding(n_items, n_factors)\n",
        "        \n",
        "        # Initialize with small random values\n",
        "        scale = 1.0 / np.sqrt(n_factors)\n",
        "        nn.init.zeros_(self.user_biases.weight)\n",
        "        nn.init.zeros_(self.item_biases.weight)\n",
        "        nn.init.normal_(self.user_factors.weight, 0, scale)\n",
        "        nn.init.normal_(self.item_factors.weight, 0, scale)\n",
        "    \n",
        "    def forward(self, user_indices: torch.Tensor, item_indices: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute predictions for user-item pairs.\"\"\"\n",
        "        user_bias = self.user_biases(user_indices).squeeze(-1)\n",
        "        item_bias = self.item_biases(item_indices).squeeze(-1)\n",
        "        user_vec = self.user_factors(user_indices)\n",
        "        item_vec = self.item_factors(item_indices)\n",
        "        \n",
        "        # Dot product of latent factors\n",
        "        interaction = torch.sum(user_vec * item_vec, dim=1)\n",
        "        \n",
        "        return self.global_mean + user_bias + item_bias + interaction\n",
        "    \n",
        "    def l2_regularization(self) -> torch.Tensor:\n",
        "        \"\"\"Compute L2 regularization term.\"\"\"\n",
        "        return (\n",
        "            torch.sum(self.user_biases.weight ** 2) +\n",
        "            torch.sum(self.item_biases.weight ** 2) +\n",
        "            torch.sum(self.user_factors.weight ** 2) +\n",
        "            torch.sum(self.item_factors.weight ** 2)\n",
        "        )\n",
        "\n",
        "\n",
        "class FunkSVDRecommender(BaseRecommender):\n",
        "    \"\"\"\n",
        "    Matrix Factorization using Mini-batch Gradient Descent with GPU acceleration.\n",
        "    \n",
        "    Jointly optimizes:\n",
        "    r_ui = mu + b_u + b_i + p_u^T * q_i\n",
        "    \n",
        "    where:\n",
        "    - mu: global mean rating\n",
        "    - b_u: user bias (learned)\n",
        "    - b_i: item bias (learned)\n",
        "    - p_u: user latent factor vector\n",
        "    - q_i: item latent factor vector\n",
        "    \n",
        "    Uses PyTorch MPS backend for GPU acceleration on Apple Silicon.\n",
        "    Mini-batch training for efficient GPU utilization.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        n_factors: int = 50,\n",
        "        lr: float = 0.005,\n",
        "        reg: float = 0.02,\n",
        "        n_epochs: int = 30,\n",
        "        batch_size: int = 1024,\n",
        "        early_stop_patience: int = 5,\n",
        "        lr_decay: float = 0.95,\n",
        "        min_lr: float = 0.0001,\n",
        "        val_fraction: float = 0.1,\n",
        "        random_state: int = 42,\n",
        "        verbose: bool = True,\n",
        "        use_gpu: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize FunkSVD recommender.\n",
        "        \n",
        "        Args:\n",
        "            n_factors: Number of latent factors\n",
        "            lr: Initial learning rate\n",
        "            reg: L2 regularization strength\n",
        "            n_epochs: Maximum number of training epochs\n",
        "            batch_size: Mini-batch size for GPU training\n",
        "            early_stop_patience: Stop if no improvement for this many epochs\n",
        "            lr_decay: Multiply learning rate by this factor each epoch\n",
        "            min_lr: Minimum learning rate (stop decaying below this)\n",
        "            val_fraction: Fraction of training data to use for validation\n",
        "            random_state: Random seed for reproducibility\n",
        "            verbose: Print training progress\n",
        "            use_gpu: Whether to use GPU acceleration (MPS on Apple Silicon)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_factors = n_factors\n",
        "        self.lr = lr\n",
        "        self.reg = reg\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.early_stop_patience = early_stop_patience\n",
        "        self.lr_decay = lr_decay\n",
        "        self.min_lr = min_lr\n",
        "        self.val_fraction = val_fraction\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        # Set device\n",
        "        if use_gpu:\n",
        "            self.device = get_device()\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "        \n",
        "        # Model (initialized in fit)\n",
        "        self.model = None\n",
        "        self.global_mean = None\n",
        "        \n",
        "        # Numpy copies for prediction (CPU)\n",
        "        self.user_biases = None\n",
        "        self.item_biases = None\n",
        "        self.user_factors = None\n",
        "        self.item_factors = None\n",
        "        \n",
        "        # Mappings\n",
        "        self.user_id_to_idx = {}\n",
        "        self.item_id_to_idx = {}\n",
        "        \n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def _compute_rmse(self, model: nn.Module, user_indices: torch.Tensor, \n",
        "                      item_indices: torch.Tensor, ratings: torch.Tensor) -> float:\n",
        "        \"\"\"Compute RMSE on given data.\"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = model(user_indices, item_indices)\n",
        "            mse = torch.mean((predictions - ratings) ** 2)\n",
        "            return torch.sqrt(mse).item()\n",
        "    \n",
        "    def fit(self, train_df: pd.DataFrame) -> 'FunkSVDRecommender':\n",
        "        \"\"\"\n",
        "        Train FunkSVD model using mini-batch SGD with GPU acceleration.\n",
        "        \n",
        "        Args:\n",
        "            train_df: DataFrame with columns [user_id, movie_id, rating]\n",
        "        \n",
        "        Returns:\n",
        "            Self (for method chaining)\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(f\"Training FunkSVD with {self.n_factors} factors on {self.device}...\")\n",
        "            print(f\"Hyperparameters: lr={self.lr}, reg={self.reg}, epochs={self.n_epochs}, batch_size={self.batch_size}\")\n",
        "        \n",
        "        torch.manual_seed(self.random_state)\n",
        "        np.random.seed(self.random_state)\n",
        "        \n",
        "        # Compute global mean\n",
        "        self.global_mean = float(train_df['rating'].mean())\n",
        "        \n",
        "        # Create mappings\n",
        "        unique_users = train_df['user_id'].unique()\n",
        "        unique_items = train_df['movie_id'].unique()\n",
        "        \n",
        "        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
        "        self.item_id_to_idx = {iid: idx for idx, iid in enumerate(unique_items)}\n",
        "        \n",
        "        n_users = len(unique_users)\n",
        "        n_items = len(unique_items)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f\"Users: {n_users:,}, Items: {n_items:,}\")\n",
        "        \n",
        "        # Convert to numpy/torch\n",
        "        user_indices = torch.from_numpy(\n",
        "            train_df['user_id'].map(self.user_id_to_idx).values.astype(np.int64)\n",
        "        )\n",
        "        item_indices = torch.from_numpy(\n",
        "            train_df['movie_id'].map(self.item_id_to_idx).values.astype(np.int64)\n",
        "        )\n",
        "        ratings = torch.from_numpy(train_df['rating'].values.astype(np.float32))\n",
        "        \n",
        "        # Split into train and validation\n",
        "        n_samples = len(ratings)\n",
        "        indices = torch.randperm(n_samples)\n",
        "        \n",
        "        use_validation = self.val_fraction > 0\n",
        "        \n",
        "        if use_validation:\n",
        "            val_size = int(n_samples * self.val_fraction)\n",
        "            val_idx = indices[:val_size]\n",
        "            train_idx = indices[val_size:]\n",
        "            \n",
        "            train_users = user_indices[train_idx].to(self.device)\n",
        "            train_items = item_indices[train_idx].to(self.device)\n",
        "            train_ratings = ratings[train_idx].to(self.device)\n",
        "            \n",
        "            val_users = user_indices[val_idx].to(self.device)\n",
        "            val_items = item_indices[val_idx].to(self.device)\n",
        "            val_ratings = ratings[val_idx].to(self.device)\n",
        "            \n",
        "            if self.verbose:\n",
        "                print(f\"Training samples: {len(train_idx):,}, Validation: {len(val_idx):,}\")\n",
        "        else:\n",
        "            train_users = user_indices.to(self.device)\n",
        "            train_items = item_indices.to(self.device)\n",
        "            train_ratings = ratings.to(self.device)\n",
        "            val_users = val_items = val_ratings = None\n",
        "            \n",
        "            if self.verbose:\n",
        "                print(f\"Training samples: {n_samples:,} (no validation split)\")\n",
        "        \n",
        "        # Create model\n",
        "        self.model = FunkSVDModel(n_users, n_items, self.n_factors, self.global_mean).to(self.device)\n",
        "        \n",
        "        # Optimizer with L2 regularization via weight_decay\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.reg)\n",
        "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.lr_decay)\n",
        "        \n",
        "        # Training loop\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        best_state = None\n",
        "        \n",
        "        n_train = len(train_users)\n",
        "        n_batches = (n_train + self.batch_size - 1) // self.batch_size\n",
        "        \n",
        "        for epoch in range(self.n_epochs):\n",
        "            self.model.train()\n",
        "            \n",
        "            # Shuffle training data\n",
        "            perm = torch.randperm(n_train, device=self.device)\n",
        "            train_users_shuf = train_users[perm]\n",
        "            train_items_shuf = train_items[perm]\n",
        "            train_ratings_shuf = train_ratings[perm]\n",
        "            \n",
        "            epoch_loss = 0.0\n",
        "            \n",
        "            for batch_idx in range(n_batches):\n",
        "                start = batch_idx * self.batch_size\n",
        "                end = min(start + self.batch_size, n_train)\n",
        "                \n",
        "                batch_users = train_users_shuf[start:end]\n",
        "                batch_items = train_items_shuf[start:end]\n",
        "                batch_ratings = train_ratings_shuf[start:end]\n",
        "                \n",
        "                # Forward pass\n",
        "                predictions = self.model(batch_users, batch_items)\n",
        "                loss = torch.mean((predictions - batch_ratings) ** 2)\n",
        "                \n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                epoch_loss += loss.item() * (end - start)\n",
        "            \n",
        "            # Compute epoch metrics\n",
        "            train_rmse = np.sqrt(epoch_loss / n_train)\n",
        "            self.train_losses.append(train_rmse)\n",
        "            \n",
        "            if use_validation:\n",
        "                val_rmse = self._compute_rmse(self.model, val_users, val_items, val_ratings)\n",
        "                self.val_losses.append(val_rmse)\n",
        "                \n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                if self.verbose:\n",
        "                    print(f\"Epoch {epoch+1}/{self.n_epochs}: Train RMSE={train_rmse:.4f}, Val RMSE={val_rmse:.4f}, LR={current_lr:.6f}\")\n",
        "                \n",
        "                # Early stopping\n",
        "                if val_rmse < best_val_loss:\n",
        "                    best_val_loss = val_rmse\n",
        "                    patience_counter = 0\n",
        "                    best_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= self.early_stop_patience:\n",
        "                        if self.verbose:\n",
        "                            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                        break\n",
        "            else:\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                if self.verbose:\n",
        "                    print(f\"Epoch {epoch+1}/{self.n_epochs}: Train RMSE={train_rmse:.4f}, LR={current_lr:.6f}\")\n",
        "            \n",
        "            # Learning rate decay\n",
        "            if current_lr > self.min_lr:\n",
        "                scheduler.step()\n",
        "        \n",
        "        # Restore best model\n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "        \n",
        "        # Copy parameters to numpy for CPU prediction\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            self.user_biases = self.model.user_biases.weight.cpu().numpy().flatten()\n",
        "            self.item_biases = self.model.item_biases.weight.cpu().numpy().flatten()\n",
        "            self.user_factors = self.model.user_factors.weight.cpu().numpy()\n",
        "            self.item_factors = self.model.item_factors.weight.cpu().numpy()\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f\"\u2713 FunkSVD training complete on {self.device}!\")\n",
        "            if use_validation:\n",
        "                print(f\"Best validation RMSE: {best_val_loss:.4f}\")\n",
        "            print(f\"Global mean: {self.global_mean:.4f}\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, user_id: int, movie_id: int) -> float:\n",
        "        \"\"\"\n",
        "        Predict rating for a user-movie pair.\n",
        "        \n",
        "        Args:\n",
        "            user_id: User identifier\n",
        "            movie_id: Movie identifier\n",
        "        \n",
        "        Returns:\n",
        "            Predicted rating\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before prediction\")\n",
        "        \n",
        "        prediction = self.global_mean\n",
        "        \n",
        "        user_idx = self.user_id_to_idx.get(user_id)\n",
        "        item_idx = self.item_id_to_idx.get(movie_id)\n",
        "        \n",
        "        if user_idx is not None:\n",
        "            prediction += self.user_biases[user_idx]\n",
        "        \n",
        "        if item_idx is not None:\n",
        "            prediction += self.item_biases[item_idx]\n",
        "        \n",
        "        if user_idx is not None and item_idx is not None:\n",
        "            prediction += np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "    def predict_batch(self, pairs: List[Tuple[int, int]]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Efficient batch prediction.\n",
        "        \n",
        "        Args:\n",
        "            pairs: List of (user_id, movie_id) tuples\n",
        "        \n",
        "        Returns:\n",
        "            Array of predicted ratings\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before prediction\")\n",
        "        \n",
        "        predictions = np.full(len(pairs), self.global_mean, dtype=np.float32)\n",
        "        \n",
        "        for i, (user_id, movie_id) in enumerate(pairs):\n",
        "            user_idx = self.user_id_to_idx.get(user_id)\n",
        "            item_idx = self.item_id_to_idx.get(movie_id)\n",
        "            \n",
        "            if user_idx is not None:\n",
        "                predictions[i] += self.user_biases[user_idx]\n",
        "            \n",
        "            if item_idx is not None:\n",
        "                predictions[i] += self.item_biases[item_idx]\n",
        "            \n",
        "            if user_idx is not None and item_idx is not None:\n",
        "                predictions[i] += np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "print('\u2713 FunkSVD recommender defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ALS Recommender (GPU Accelerated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ALSRecommender(BaseRecommender):\n",
        "    \"\"\"\n",
        "    Matrix Factorization using Alternating Least Squares with GPU acceleration.\n",
        "    \n",
        "    Prediction formula:\n",
        "    r_ui = mu + b_u + b_i + p_u^T * q_i\n",
        "    \n",
        "    Algorithm:\n",
        "    1. Fix Q (item factors), solve for P (user factors) in closed form\n",
        "    2. Fix P (user factors), solve for Q (item factors) in closed form\n",
        "    3. Repeat until convergence\n",
        "    \n",
        "    The closed-form solution with regularization:\n",
        "    p_u = (Q^T * Q + lambda * I)^-1 * Q^T * r_u\n",
        "    \n",
        "    Uses PyTorch MPS backend for GPU acceleration on Apple Silicon.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        n_factors: int = 50,\n",
        "        reg: float = 0.1,\n",
        "        n_iterations: int = 15,\n",
        "        random_state: int = 42,\n",
        "        verbose: bool = True,\n",
        "        use_gpu: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize ALS recommender.\n",
        "        \n",
        "        Args:\n",
        "            n_factors: Number of latent factors\n",
        "            reg: L2 regularization strength (higher = more regularization)\n",
        "            n_iterations: Number of alternating iterations\n",
        "            random_state: Random seed for reproducibility\n",
        "            verbose: Print training progress\n",
        "            use_gpu: Whether to use GPU acceleration (MPS on Apple Silicon)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_factors = n_factors\n",
        "        self.reg = reg\n",
        "        self.n_iterations = n_iterations\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        # Set device\n",
        "        if use_gpu:\n",
        "            self.device = get_device()\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "        \n",
        "        # Model parameters (stored as numpy for compatibility)\n",
        "        self.global_mean = None\n",
        "        self.user_biases = None\n",
        "        self.item_biases = None\n",
        "        self.user_factors = None\n",
        "        self.item_factors = None\n",
        "        \n",
        "        # Mappings\n",
        "        self.user_id_to_idx = {}\n",
        "        self.item_id_to_idx = {}\n",
        "        \n",
        "        # Training history\n",
        "        self.losses = []\n",
        "    \n",
        "    def _compute_rmse_gpu(self, user_indices: torch.Tensor, item_indices: torch.Tensor, \n",
        "                          ratings: torch.Tensor, user_factors: torch.Tensor, \n",
        "                          item_factors: torch.Tensor, user_biases: torch.Tensor,\n",
        "                          item_biases: torch.Tensor) -> float:\n",
        "        \"\"\"Compute RMSE on GPU.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            predictions = (\n",
        "                self.global_mean +\n",
        "                user_biases[user_indices] +\n",
        "                item_biases[item_indices] +\n",
        "                torch.sum(user_factors[user_indices] * item_factors[item_indices], dim=1)\n",
        "            )\n",
        "            mse = torch.mean((ratings - predictions) ** 2)\n",
        "            return torch.sqrt(mse).item()\n",
        "    \n",
        "    def fit(self, train_df: pd.DataFrame) -> 'ALSRecommender':\n",
        "        \"\"\"\n",
        "        Train ALS model with GPU acceleration.\n",
        "        \n",
        "        Args:\n",
        "            train_df: DataFrame with columns [user_id, movie_id, rating]\n",
        "        \n",
        "        Returns:\n",
        "            Self (for method chaining)\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(f\"Training ALS with {self.n_factors} factors on {self.device}...\")\n",
        "            print(f\"Hyperparameters: reg={self.reg}, iterations={self.n_iterations}\")\n",
        "        \n",
        "        torch.manual_seed(self.random_state)\n",
        "        np.random.seed(self.random_state)\n",
        "        \n",
        "        # Compute global mean\n",
        "        self.global_mean = float(train_df['rating'].mean())\n",
        "        \n",
        "        # Create mappings\n",
        "        unique_users = train_df['user_id'].unique()\n",
        "        unique_items = train_df['movie_id'].unique()\n",
        "        \n",
        "        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
        "        self.item_id_to_idx = {iid: idx for idx, iid in enumerate(unique_items)}\n",
        "        \n",
        "        n_users = len(unique_users)\n",
        "        n_items = len(unique_items)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f\"Users: {n_users:,}, Items: {n_items:,}\")\n",
        "        \n",
        "        # Compute biases\n",
        "        user_means = train_df.groupby('user_id')['rating'].mean()\n",
        "        user_biases_np = np.zeros(n_users, dtype=np.float32)\n",
        "        for uid, mean in user_means.items():\n",
        "            user_biases_np[self.user_id_to_idx[uid]] = mean - self.global_mean\n",
        "        \n",
        "        item_means = train_df.groupby('movie_id')['rating'].mean()\n",
        "        item_biases_np = np.zeros(n_items, dtype=np.float32)\n",
        "        for iid, mean in item_means.items():\n",
        "            item_biases_np[self.item_id_to_idx[iid]] = mean - self.global_mean\n",
        "        \n",
        "        # Create index arrays\n",
        "        user_indices_np = train_df['user_id'].map(self.user_id_to_idx).values.astype(np.int64)\n",
        "        item_indices_np = train_df['movie_id'].map(self.item_id_to_idx).values.astype(np.int64)\n",
        "        ratings_np = train_df['rating'].values.astype(np.float32)\n",
        "        \n",
        "        # Move to GPU\n",
        "        user_indices = torch.from_numpy(user_indices_np).to(self.device)\n",
        "        item_indices = torch.from_numpy(item_indices_np).to(self.device)\n",
        "        ratings = torch.from_numpy(ratings_np).to(self.device)\n",
        "        user_biases = torch.from_numpy(user_biases_np).to(self.device)\n",
        "        item_biases = torch.from_numpy(item_biases_np).to(self.device)\n",
        "        \n",
        "        # Normalized ratings (subtract global mean and biases)\n",
        "        ratings_normalized = (\n",
        "            ratings - \n",
        "            self.global_mean - \n",
        "            user_biases[user_indices] - \n",
        "            item_biases[item_indices]\n",
        "        )\n",
        "        \n",
        "        # Build user->items and item->users mappings for efficient access\n",
        "        user_to_items = [[] for _ in range(n_users)]\n",
        "        user_to_ratings = [[] for _ in range(n_users)]\n",
        "        item_to_users = [[] for _ in range(n_items)]\n",
        "        item_to_ratings = [[] for _ in range(n_items)]\n",
        "        \n",
        "        for idx in range(len(train_df)):\n",
        "            u = user_indices_np[idx]\n",
        "            i = item_indices_np[idx]\n",
        "            r = float(ratings_normalized[idx].cpu())\n",
        "            user_to_items[u].append(i)\n",
        "            user_to_ratings[u].append(r)\n",
        "            item_to_users[i].append(u)\n",
        "            item_to_ratings[i].append(r)\n",
        "        \n",
        "        if self.verbose:\n",
        "            sparsity = 1 - len(train_df) / (n_users * n_items)\n",
        "            print(f\"Matrix sparsity: {sparsity:.4f}\")\n",
        "        \n",
        "        # Initialize factors randomly on GPU\n",
        "        scale = 1.0 / np.sqrt(self.n_factors)\n",
        "        user_factors = torch.randn(n_users, self.n_factors, device=self.device, dtype=torch.float32) * scale\n",
        "        item_factors = torch.randn(n_items, self.n_factors, device=self.device, dtype=torch.float32) * scale\n",
        "        \n",
        "        # Regularization matrix\n",
        "        reg_matrix = self.reg * torch.eye(self.n_factors, device=self.device, dtype=torch.float32)\n",
        "        \n",
        "        # Alternating optimization\n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Step 1: Fix item factors, update user factors\n",
        "            for u in range(n_users):\n",
        "                item_idxs = user_to_items[u]\n",
        "                if len(item_idxs) == 0:\n",
        "                    continue\n",
        "                \n",
        "                item_idxs_t = torch.tensor(item_idxs, dtype=torch.long, device=self.device)\n",
        "                user_ratings_t = torch.tensor(user_to_ratings[u], dtype=torch.float32, device=self.device)\n",
        "                \n",
        "                Q_u = item_factors[item_idxs_t]  # (n_rated, n_factors)\n",
        "                A = Q_u.T @ Q_u + reg_matrix     # (n_factors, n_factors)\n",
        "                b = Q_u.T @ user_ratings_t       # (n_factors,)\n",
        "                \n",
        "                user_factors[u] = torch.linalg.solve(A, b)\n",
        "            \n",
        "            # Step 2: Fix user factors, update item factors\n",
        "            for i in range(n_items):\n",
        "                user_idxs = item_to_users[i]\n",
        "                if len(user_idxs) == 0:\n",
        "                    continue\n",
        "                \n",
        "                user_idxs_t = torch.tensor(user_idxs, dtype=torch.long, device=self.device)\n",
        "                item_ratings_t = torch.tensor(item_to_ratings[i], dtype=torch.float32, device=self.device)\n",
        "                \n",
        "                P_i = user_factors[user_idxs_t]  # (n_rated, n_factors)\n",
        "                A = P_i.T @ P_i + reg_matrix     # (n_factors, n_factors)\n",
        "                b = P_i.T @ item_ratings_t       # (n_factors,)\n",
        "                \n",
        "                item_factors[i] = torch.linalg.solve(A, b)\n",
        "            \n",
        "            # Compute loss\n",
        "            rmse = self._compute_rmse_gpu(user_indices, item_indices, ratings,\n",
        "                                          user_factors, item_factors, user_biases, item_biases)\n",
        "            self.losses.append(rmse)\n",
        "            \n",
        "            if self.verbose:\n",
        "                print(f\"Iteration {iteration+1}/{self.n_iterations}: RMSE={rmse:.4f}\")\n",
        "        \n",
        "        # Move final parameters back to CPU/numpy for prediction compatibility\n",
        "        self.user_factors = user_factors.cpu().numpy()\n",
        "        self.item_factors = item_factors.cpu().numpy()\n",
        "        self.user_biases = user_biases.cpu().numpy()\n",
        "        self.item_biases = item_biases.cpu().numpy()\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f\"\u2713 ALS training complete on {self.device}!\")\n",
        "            print(f\"Final RMSE: {self.losses[-1]:.4f}\")\n",
        "            print(f\"Global mean: {self.global_mean:.4f}\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, user_id: int, movie_id: int) -> float:\n",
        "        \"\"\"\n",
        "        Predict rating for a user-movie pair.\n",
        "        \n",
        "        Args:\n",
        "            user_id: User identifier\n",
        "            movie_id: Movie identifier\n",
        "        \n",
        "        Returns:\n",
        "            Predicted rating\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before prediction\")\n",
        "        \n",
        "        # Start with global mean\n",
        "        prediction = self.global_mean\n",
        "        \n",
        "        # Add biases\n",
        "        user_idx = self.user_id_to_idx.get(user_id)\n",
        "        item_idx = self.item_id_to_idx.get(movie_id)\n",
        "        \n",
        "        if user_idx is not None:\n",
        "            prediction += self.user_biases[user_idx]\n",
        "        \n",
        "        if item_idx is not None:\n",
        "            prediction += self.item_biases[item_idx]\n",
        "        \n",
        "        # Add latent factor interaction\n",
        "        if user_idx is not None and item_idx is not None:\n",
        "            prediction += np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "    def predict_batch(self, pairs: List[Tuple[int, int]]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Efficient batch prediction.\n",
        "        \n",
        "        Args:\n",
        "            pairs: List of (user_id, movie_id) tuples\n",
        "        \n",
        "        Returns:\n",
        "            Array of predicted ratings\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before prediction\")\n",
        "        \n",
        "        predictions = np.full(len(pairs), self.global_mean, dtype=np.float32)\n",
        "        \n",
        "        for i, (user_id, movie_id) in enumerate(pairs):\n",
        "            user_idx = self.user_id_to_idx.get(user_id)\n",
        "            item_idx = self.item_id_to_idx.get(movie_id)\n",
        "            \n",
        "            if user_idx is not None:\n",
        "                predictions[i] += self.user_biases[user_idx]\n",
        "            \n",
        "            if item_idx is not None:\n",
        "                predictions[i] += self.item_biases[item_idx]\n",
        "            \n",
        "            if user_idx is not None and item_idx is not None:\n",
        "                predictions[i] += np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "print('\u2713 ALS recommender defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Hybrid Recommender (CF + Content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridRecommender(BaseRecommender):\n",
        "    \"\"\"\n",
        "    Hybrid recommender combining collaborative filtering with content features.\n",
        "    \n",
        "    Prediction formula:\n",
        "    r_ui = mu + b_u + b_i + p_u^T * q_i + w^T * genre_features_i\n",
        "    \n",
        "    where:\n",
        "    - mu: global mean rating\n",
        "    - b_u: user bias\n",
        "    - b_i: item bias\n",
        "    - p_u, q_i: latent factors (collaborative)\n",
        "    - w: genre weight vector\n",
        "    - genre_features_i: binary genre vector for item i\n",
        "    \n",
        "    Benefits:\n",
        "    - Better cold-start handling (can use genres for new items)\n",
        "    - Content features act as additional regularization\n",
        "    - Combines strengths of both approaches\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        n_factors: int = 50,\n",
        "        lr: float = 0.005,\n",
        "        reg: float = 0.02,\n",
        "        content_reg: float = 0.01,\n",
        "        n_epochs: int = 30,\n",
        "        early_stop_patience: int = 5,\n",
        "        lr_decay: float = 0.95,\n",
        "        min_lr: float = 0.0001,\n",
        "        val_fraction: float = 0.1,\n",
        "        genre_features: Optional[Dict[int, np.ndarray]] = None,\n",
        "        random_state: int = 42,\n",
        "        verbose: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize Hybrid recommender.\n",
        "        \n",
        "        Args:\n",
        "            n_factors: Number of latent factors\n",
        "            lr: Initial learning rate\n",
        "            reg: L2 regularization for latent factors and biases\n",
        "            content_reg: L2 regularization for genre weights\n",
        "            n_epochs: Maximum number of training epochs\n",
        "            early_stop_patience: Stop if no improvement for this many epochs\n",
        "            lr_decay: Multiply learning rate by this factor each epoch\n",
        "            min_lr: Minimum learning rate\n",
        "            val_fraction: Fraction of training data for validation\n",
        "            genre_features: Dict mapping movie_id to binary genre vector\n",
        "            random_state: Random seed\n",
        "            verbose: Print training progress\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_factors = n_factors\n",
        "        self.lr = lr\n",
        "        self.reg = reg\n",
        "        self.content_reg = content_reg\n",
        "        self.n_epochs = n_epochs\n",
        "        self.early_stop_patience = early_stop_patience\n",
        "        self.lr_decay = lr_decay\n",
        "        self.min_lr = min_lr\n",
        "        self.val_fraction = val_fraction\n",
        "        self.genre_features = genre_features\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        \n",
        "        # Model parameters\n",
        "        self.global_mean = None\n",
        "        self.user_biases = None\n",
        "        self.item_biases = None\n",
        "        self.user_factors = None\n",
        "        self.item_factors = None\n",
        "        self.genre_weights = None\n",
        "        \n",
        "        # Genre feature info\n",
        "        self.n_genres = None\n",
        "        self.internal_genre_features = None  # Numpy array for fast access\n",
        "        \n",
        "        # Mappings\n",
        "        self.user_id_to_idx = {}\n",
        "        self.item_id_to_idx = {}\n",
        "        \n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "    \n",
        "    def _extract_genres_from_df(self, train_df: pd.DataFrame) -> Dict[int, np.ndarray]:\n",
        "        \"\"\"Extract genre features from training data if not provided.\"\"\"\n",
        "        # Get unique movies and their genres (would need movies_df for real implementation)\n",
        "        # This is a fallback - ideally genre_features is provided\n",
        "        unique_items = train_df['movie_id'].unique()\n",
        "        # Return empty features (no genres)\n",
        "        return {iid: np.array([], dtype=np.float32) for iid in unique_items}\n",
        "    \n",
        "    def _init_parameters(self, n_users: int, n_items: int):\n",
        "        \"\"\"Initialize model parameters.\"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "        \n",
        "        # Biases\n",
        "        self.user_biases = np.zeros(n_users, dtype=np.float32)\n",
        "        self.item_biases = np.zeros(n_items, dtype=np.float32)\n",
        "        \n",
        "        # Latent factors\n",
        "        scale = 1.0 / np.sqrt(self.n_factors)\n",
        "        self.user_factors = np.random.normal(0, scale, (n_users, self.n_factors)).astype(np.float32)\n",
        "        self.item_factors = np.random.normal(0, scale, (n_items, self.n_factors)).astype(np.float32)\n",
        "        \n",
        "        # Genre weights\n",
        "        if self.n_genres > 0:\n",
        "            self.genre_weights = np.zeros(self.n_genres, dtype=np.float32)\n",
        "    \n",
        "    def _get_genre_contribution(self, item_idx: int) -> float:\n",
        "        \"\"\"Get genre-based contribution to prediction.\"\"\"\n",
        "        if self.n_genres == 0 or self.genre_weights is None:\n",
        "            return 0.0\n",
        "        \n",
        "        genre_vec = self.internal_genre_features[item_idx]\n",
        "        return np.dot(self.genre_weights, genre_vec)\n",
        "    \n",
        "    def _compute_loss(self, user_indices: np.ndarray, item_indices: np.ndarray,\n",
        "                      ratings: np.ndarray) -> float:\n",
        "        \"\"\"Compute RMSE loss.\"\"\"\n",
        "        predictions = (\n",
        "            self.global_mean +\n",
        "            self.user_biases[user_indices] +\n",
        "            self.item_biases[item_indices] +\n",
        "            np.sum(self.user_factors[user_indices] * self.item_factors[item_indices], axis=1)\n",
        "        )\n",
        "        \n",
        "        # Add genre contribution\n",
        "        if self.n_genres > 0 and self.genre_weights is not None:\n",
        "            genre_contrib = np.sum(\n",
        "                self.internal_genre_features[item_indices] * self.genre_weights,\n",
        "                axis=1\n",
        "            )\n",
        "            predictions += genre_contrib\n",
        "        \n",
        "        mse = np.mean((predictions - ratings) ** 2)\n",
        "        return np.sqrt(mse)\n",
        "    \n",
        "    def fit(self, train_df: pd.DataFrame) -> 'HybridRecommender':\n",
        "        \"\"\"\n",
        "        Train hybrid model using SGD.\n",
        "        \n",
        "        Args:\n",
        "            train_df: DataFrame with columns [user_id, movie_id, rating]\n",
        "        \n",
        "        Returns:\n",
        "            Self (for method chaining)\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(f\"Training Hybrid (CF + Content) with {self.n_factors} factors...\")\n",
        "            print(f\"Hyperparameters: lr={self.lr}, reg={self.reg}, content_reg={self.content_reg}\")\n",
        "        \n",
        "        # Compute global mean\n",
        "        self.global_mean = train_df['rating'].mean()\n",
        "        \n",
        "        # Create mappings\n",
        "        unique_users = train_df['user_id'].unique()\n",
        "        unique_items = train_df['movie_id'].unique()\n",
        "        \n",
        "        self.user_id_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
        "        self.item_id_to_idx = {iid: idx for idx, iid in enumerate(unique_items)}\n",
        "        \n",
        "        n_users = len(unique_users)\n",
        "        n_items = len(unique_items)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f\"Users: {n_users:,}, Items: {n_items:,}\")\n",
        "        \n",
        "        # Setup genre features\n",
        "        if self.genre_features is None:\n",
        "            if self.verbose:\n",
        "                print(\"No genre features provided, running without content features\")\n",
        "            self.n_genres = 0\n",
        "            self.internal_genre_features = np.zeros((n_items, 0), dtype=np.float32)\n",
        "        else:\n",
        "            # Build internal genre feature matrix\n",
        "            sample_vec = next(iter(self.genre_features.values()))\n",
        "            self.n_genres = len(sample_vec)\n",
        "            self.internal_genre_features = np.zeros((n_items, self.n_genres), dtype=np.float32)\n",
        "            \n",
        "            for iid, idx in self.item_id_to_idx.items():\n",
        "                if iid in self.genre_features:\n",
        "                    self.internal_genre_features[idx] = self.genre_features[iid]\n",
        "            \n",
        "            if self.verbose:\n",
        "                print(f\"Using {self.n_genres} genre features\")\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self._init_parameters(n_users, n_items)\n",
        "        \n",
        "        # Convert to numpy arrays\n",
        "        user_indices = train_df['user_id'].map(self.user_id_to_idx).values.astype(np.int32)\n",
        "        item_indices = train_df['movie_id'].map(self.item_id_to_idx).values.astype(np.int32)\n",
        "        ratings = train_df['rating'].values.astype(np.float32)\n",
        "        \n",
        "        # Train/validation split\n",
        "        n_samples = len(ratings)\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        \n",
        "        # Handle val_fraction=0 (no internal validation)\n",
        "        use_validation = self.val_fraction > 0\n",
        "        \n",
        "        if use_validation:\n",
        "            val_size = int(n_samples * self.val_fraction)\n",
        "            val_indices = indices[:val_size]\n",
        "            train_indices = indices[val_size:]\n",
        "            \n",
        "            train_users = user_indices[train_indices]\n",
        "            train_items = item_indices[train_indices]\n",
        "            train_ratings = ratings[train_indices]\n",
        "            \n",
        "            val_users = user_indices[val_indices]\n",
        "            val_items = item_indices[val_indices]\n",
        "            val_ratings = ratings[val_indices]\n",
        "            \n",
        "            if self.verbose:\n",
        "                print(f\"Training samples: {len(train_indices):,}, Validation: {len(val_indices):,}\")\n",
        "        else:\n",
        "            train_users = user_indices\n",
        "            train_items = item_indices\n",
        "            train_ratings = ratings\n",
        "            val_users = None\n",
        "            val_items = None\n",
        "            val_ratings = None\n",
        "            \n",
        "            if self.verbose:\n",
        "                print(f\"Training samples: {n_samples:,} (no validation split)\")\n",
        "        \n",
        "        # Training loop\n",
        "        current_lr = self.lr\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        best_params = None\n",
        "        \n",
        "        for epoch in range(self.n_epochs):\n",
        "            # Shuffle training data\n",
        "            perm = np.random.permutation(len(train_ratings))\n",
        "            train_users_shuffled = train_users[perm]\n",
        "            train_items_shuffled = train_items[perm]\n",
        "            train_ratings_shuffled = train_ratings[perm]\n",
        "            \n",
        "            # SGD updates\n",
        "            for u, i, r in zip(train_users_shuffled, train_items_shuffled, train_ratings_shuffled):\n",
        "                # Compute prediction\n",
        "                pred = (\n",
        "                    self.global_mean +\n",
        "                    self.user_biases[u] +\n",
        "                    self.item_biases[i] +\n",
        "                    np.dot(self.user_factors[u], self.item_factors[i])\n",
        "                )\n",
        "                \n",
        "                # Add genre contribution\n",
        "                if self.n_genres > 0:\n",
        "                    pred += np.dot(self.genre_weights, self.internal_genre_features[i])\n",
        "                \n",
        "                # Compute error\n",
        "                error = r - pred\n",
        "                \n",
        "                # Update biases\n",
        "                self.user_biases[u] += current_lr * (error - self.reg * self.user_biases[u])\n",
        "                self.item_biases[i] += current_lr * (error - self.reg * self.item_biases[i])\n",
        "                \n",
        "                # Update latent factors\n",
        "                user_factor_old = self.user_factors[u].copy()\n",
        "                self.user_factors[u] += current_lr * (error * self.item_factors[i] - self.reg * self.user_factors[u])\n",
        "                self.item_factors[i] += current_lr * (error * user_factor_old - self.reg * self.item_factors[i])\n",
        "                \n",
        "                # Update genre weights\n",
        "                if self.n_genres > 0:\n",
        "                    genre_vec = self.internal_genre_features[i]\n",
        "                    self.genre_weights += current_lr * (error * genre_vec - self.content_reg * self.genre_weights)\n",
        "            \n",
        "            # Compute losses\n",
        "            train_loss = self._compute_loss(train_users, train_items, train_ratings)\n",
        "            self.train_losses.append(train_loss)\n",
        "            \n",
        "            if use_validation:\n",
        "                val_loss = self._compute_loss(val_users, val_items, val_ratings)\n",
        "                self.val_losses.append(val_loss)\n",
        "                \n",
        "                if self.verbose:\n",
        "                    print(f\"Epoch {epoch+1}/{self.n_epochs}: Train RMSE={train_loss:.4f}, Val RMSE={val_loss:.4f}, LR={current_lr:.6f}\")\n",
        "                \n",
        "                # Early stopping\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                    best_params = {\n",
        "                        'user_biases': self.user_biases.copy(),\n",
        "                        'item_biases': self.item_biases.copy(),\n",
        "                        'user_factors': self.user_factors.copy(),\n",
        "                        'item_factors': self.item_factors.copy(),\n",
        "                        'genre_weights': self.genre_weights.copy() if self.genre_weights is not None else None\n",
        "                    }\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= self.early_stop_patience:\n",
        "                        if self.verbose:\n",
        "                            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                        break\n",
        "            else:\n",
        "                # No validation - just train for all epochs\n",
        "                if self.verbose:\n",
        "                    print(f\"Epoch {epoch+1}/{self.n_epochs}: Train RMSE={train_loss:.4f}, LR={current_lr:.6f}\")\n",
        "            \n",
        "            # Learning rate decay\n",
        "            current_lr = max(current_lr * self.lr_decay, self.min_lr)\n",
        "        \n",
        "        # Restore best parameters (only if we used validation)\n",
        "        if use_validation and best_params is not None:\n",
        "            self.user_biases = best_params['user_biases']\n",
        "            self.item_biases = best_params['item_biases']\n",
        "            self.user_factors = best_params['user_factors']\n",
        "            self.item_factors = best_params['item_factors']\n",
        "            self.genre_weights = best_params['genre_weights']\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f\"\u2713 Hybrid training complete!\")\n",
        "            if use_validation:\n",
        "                print(f\"Best validation RMSE: {best_val_loss:.4f}\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, user_id: int, movie_id: int) -> float:\n",
        "        \"\"\"\n",
        "        Predict rating for a user-movie pair.\n",
        "        \n",
        "        Args:\n",
        "            user_id: User identifier\n",
        "            movie_id: Movie identifier\n",
        "        \n",
        "        Returns:\n",
        "            Predicted rating\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before prediction\")\n",
        "        \n",
        "        prediction = self.global_mean\n",
        "        \n",
        "        user_idx = self.user_id_to_idx.get(user_id)\n",
        "        item_idx = self.item_id_to_idx.get(movie_id)\n",
        "        \n",
        "        if user_idx is not None:\n",
        "            prediction += self.user_biases[user_idx]\n",
        "        \n",
        "        if item_idx is not None:\n",
        "            prediction += self.item_biases[item_idx]\n",
        "            # Add genre contribution even for unknown users (cold-start)\n",
        "            if self.n_genres > 0 and self.genre_weights is not None:\n",
        "                prediction += np.dot(self.genre_weights, self.internal_genre_features[item_idx])\n",
        "        \n",
        "        if user_idx is not None and item_idx is not None:\n",
        "            prediction += np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "    def predict_batch(self, pairs: List[Tuple[int, int]]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Efficient batch prediction.\n",
        "        \n",
        "        Args:\n",
        "            pairs: List of (user_id, movie_id) tuples\n",
        "        \n",
        "        Returns:\n",
        "            Array of predicted ratings\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before prediction\")\n",
        "        \n",
        "        predictions = np.full(len(pairs), self.global_mean, dtype=np.float32)\n",
        "        \n",
        "        for i, (user_id, movie_id) in enumerate(pairs):\n",
        "            user_idx = self.user_id_to_idx.get(user_id)\n",
        "            item_idx = self.item_id_to_idx.get(movie_id)\n",
        "            \n",
        "            if user_idx is not None:\n",
        "                predictions[i] += self.user_biases[user_idx]\n",
        "            \n",
        "            if item_idx is not None:\n",
        "                predictions[i] += self.item_biases[item_idx]\n",
        "                if self.n_genres > 0 and self.genre_weights is not None:\n",
        "                    predictions[i] += np.dot(self.genre_weights, self.internal_genre_features[item_idx])\n",
        "            \n",
        "            if user_idx is not None and item_idx is not None:\n",
        "                predictions[i] += np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "print('\u2713 Hybrid recommender defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureEngineer:\n",
        "    \"\"\"\n",
        "    Comprehensive feature extraction for recommendation.\n",
        "    \n",
        "    Feature groups:\n",
        "    - User statistics (mean, std, count, percentiles)\n",
        "    - Movie statistics (mean, std, count, popularity tier)\n",
        "    - Genre features (user preferences, movie genres)\n",
        "    - Tag features (TF-IDF embeddings)\n",
        "    - Cross features (user-movie interactions)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        n_tag_components: int = 50,\n",
        "        n_genre_components: int = 20,\n",
        "        verbose: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize feature engineer.\n",
        "        \n",
        "        Args:\n",
        "            n_tag_components: Dimensionality of tag embeddings\n",
        "            n_genre_components: Number of genre features\n",
        "            verbose: Print progress\n",
        "        \"\"\"\n",
        "        self.n_tag_components = n_tag_components\n",
        "        self.n_genre_components = n_genre_components\n",
        "        self.verbose = verbose\n",
        "        \n",
        "        # Will be populated during fit\n",
        "        self.user_stats = {}\n",
        "        self.movie_stats = {}\n",
        "        self.user_genre_prefs = {}  # user -> genre preference vector\n",
        "        self.movie_genres = {}  # movie -> genre vector\n",
        "        self.movie_tag_embeddings = {}  # movie -> tag embedding\n",
        "        self.user_tag_prefs = {}  # user -> tag preference vector\n",
        "        \n",
        "        self.global_mean = 3.5\n",
        "        self.genre_list = []\n",
        "        self.tag_to_idx = {}\n",
        "        \n",
        "        self.is_fitted = False\n",
        "    \n",
        "    def fit(\n",
        "        self,\n",
        "        train_df: pd.DataFrame,\n",
        "        movies_df: pd.DataFrame,\n",
        "        tags_df: Optional[pd.DataFrame] = None\n",
        "    ) -> 'FeatureEngineer':\n",
        "        \"\"\"\n",
        "        Fit feature engineer on training data.\n",
        "        \n",
        "        Args:\n",
        "            train_df: Training ratings [user_id, movie_id, rating]\n",
        "            movies_df: Movie metadata [movie_id, title, genres]\n",
        "            tags_df: Optional tags [user_id, movie_id, tag]\n",
        "        \n",
        "        Returns:\n",
        "            Self\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(\"Fitting FeatureEngineer...\")\n",
        "        \n",
        "        self.global_mean = train_df['rating'].mean()\n",
        "        \n",
        "        # 1. User statistics\n",
        "        if self.verbose:\n",
        "            print(\"  Computing user statistics...\")\n",
        "        self._compute_user_stats(train_df)\n",
        "        \n",
        "        # 2. Movie statistics\n",
        "        if self.verbose:\n",
        "            print(\"  Computing movie statistics...\")\n",
        "        self._compute_movie_stats(train_df)\n",
        "        \n",
        "        # 3. Genre features\n",
        "        if self.verbose:\n",
        "            print(\"  Processing genre features...\")\n",
        "        self._process_genres(movies_df, train_df)\n",
        "        \n",
        "        # 4. Tag features\n",
        "        if tags_df is not None:\n",
        "            if self.verbose:\n",
        "                print(\"  Processing tag features...\")\n",
        "            self._process_tags(tags_df, train_df)\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        if self.verbose:\n",
        "            print(\"\u2713 FeatureEngineer fitted!\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def _compute_user_stats(self, train_df: pd.DataFrame):\n",
        "        \"\"\"Compute user-level statistics.\"\"\"\n",
        "        user_groups = train_df.groupby('user_id')\n",
        "        \n",
        "        for user_id, group in user_groups:\n",
        "            ratings = group['rating'].values\n",
        "            self.user_stats[user_id] = {\n",
        "                'mean': np.mean(ratings),\n",
        "                'std': np.std(ratings) if len(ratings) > 1 else 0.0,\n",
        "                'count': len(ratings),\n",
        "                'min': np.min(ratings),\n",
        "                'max': np.max(ratings),\n",
        "                'median': np.median(ratings),\n",
        "                'q25': np.percentile(ratings, 25),\n",
        "                'q75': np.percentile(ratings, 75),\n",
        "                'range': np.max(ratings) - np.min(ratings),\n",
        "                'skew': self._safe_skew(ratings),\n",
        "            }\n",
        "    \n",
        "    def _compute_movie_stats(self, train_df: pd.DataFrame):\n",
        "        \"\"\"Compute movie-level statistics.\"\"\"\n",
        "        movie_groups = train_df.groupby('movie_id')\n",
        "        \n",
        "        # Global movie stats for popularity tiers\n",
        "        movie_counts = train_df.groupby('movie_id').size()\n",
        "        count_percentiles = [\n",
        "            np.percentile(movie_counts, p) for p in [25, 50, 75, 90, 95, 99]\n",
        "        ]\n",
        "        \n",
        "        for movie_id, group in movie_groups:\n",
        "            ratings = group['rating'].values\n",
        "            count = len(ratings)\n",
        "            \n",
        "            # Popularity tier (0-5)\n",
        "            tier = sum(count > p for p in count_percentiles)\n",
        "            \n",
        "            self.movie_stats[movie_id] = {\n",
        "                'mean': np.mean(ratings),\n",
        "                'std': np.std(ratings) if len(ratings) > 1 else 0.0,\n",
        "                'count': count,\n",
        "                'log_count': np.log1p(count),\n",
        "                'min': np.min(ratings),\n",
        "                'max': np.max(ratings),\n",
        "                'median': np.median(ratings),\n",
        "                'q25': np.percentile(ratings, 25),\n",
        "                'q75': np.percentile(ratings, 75),\n",
        "                'range': np.max(ratings) - np.min(ratings),\n",
        "                'skew': self._safe_skew(ratings),\n",
        "                'popularity_tier': tier,\n",
        "                'is_rare': int(count < 50),\n",
        "                'is_popular': int(count > count_percentiles[4]),  # > 95th percentile\n",
        "            }\n",
        "    \n",
        "    def _safe_skew(self, arr: np.ndarray) -> float:\n",
        "        \"\"\"Compute skewness safely.\"\"\"\n",
        "        if len(arr) < 3:\n",
        "            return 0.0\n",
        "        std = np.std(arr)\n",
        "        if std == 0:\n",
        "            return 0.0\n",
        "        return float(np.mean(((arr - np.mean(arr)) / std) ** 3))\n",
        "    \n",
        "    def _process_genres(self, movies_df: pd.DataFrame, train_df: pd.DataFrame):\n",
        "        \"\"\"Process genre features for movies and users.\"\"\"\n",
        "        # Extract all genres\n",
        "        all_genres = set()\n",
        "        for genres_str in movies_df['genres'].dropna():\n",
        "            if isinstance(genres_str, str):\n",
        "                all_genres.update(genres_str.split('|'))\n",
        "        \n",
        "        self.genre_list = sorted(list(all_genres - {'(no genres listed)'}))\n",
        "        n_genres = len(self.genre_list)\n",
        "        genre_to_idx = {g: i for i, g in enumerate(self.genre_list)}\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f\"    Found {n_genres} genres\")\n",
        "        \n",
        "        # Movie genre vectors (one-hot)\n",
        "        for _, row in movies_df.iterrows():\n",
        "            movie_id = row['movie_id']\n",
        "            genre_vec = np.zeros(n_genres, dtype=np.float32)\n",
        "            \n",
        "            if pd.notna(row['genres']) and isinstance(row['genres'], str):\n",
        "                for genre in row['genres'].split('|'):\n",
        "                    if genre in genre_to_idx:\n",
        "                        genre_vec[genre_to_idx[genre]] = 1.0\n",
        "            \n",
        "            self.movie_genres[movie_id] = genre_vec\n",
        "        \n",
        "        # User genre preferences (weighted average of movie genres by rating)\n",
        "        user_genre_sums = defaultdict(lambda: np.zeros(n_genres, dtype=np.float32))\n",
        "        user_genre_counts = defaultdict(lambda: np.zeros(n_genres, dtype=np.float32))\n",
        "        \n",
        "        for _, row in train_df.iterrows():\n",
        "            user_id = row['user_id']\n",
        "            movie_id = row['movie_id']\n",
        "            rating = row['rating']\n",
        "            \n",
        "            if movie_id in self.movie_genres:\n",
        "                genre_vec = self.movie_genres[movie_id]\n",
        "                # Weight by rating deviation from mean\n",
        "                weight = rating - self.global_mean\n",
        "                user_genre_sums[user_id] += genre_vec * weight\n",
        "                user_genre_counts[user_id] += genre_vec\n",
        "        \n",
        "        for user_id in user_genre_sums:\n",
        "            counts = user_genre_counts[user_id]\n",
        "            counts[counts == 0] = 1  # Avoid division by zero\n",
        "            self.user_genre_prefs[user_id] = user_genre_sums[user_id] / counts\n",
        "    \n",
        "    def _process_tags(self, tags_df: pd.DataFrame, train_df: pd.DataFrame):\n",
        "        \"\"\"Process tag features using TF-IDF + SVD.\"\"\"\n",
        "        # Normalize tags\n",
        "        tags_df = tags_df.copy()\n",
        "        tags_df['tag'] = tags_df['tag'].astype(str).str.lower().str.strip()\n",
        "        \n",
        "        # Filter to frequent tags\n",
        "        tag_counts = tags_df['tag'].value_counts()\n",
        "        min_count = 5\n",
        "        valid_tags = tag_counts[tag_counts >= min_count].head(500).index\n",
        "        self.tag_to_idx = {tag: idx for idx, tag in enumerate(valid_tags)}\n",
        "        n_tags = len(self.tag_to_idx)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f\"    Using {n_tags} tags\")\n",
        "        \n",
        "        # Filter tags\n",
        "        tags_df = tags_df[tags_df['tag'].isin(self.tag_to_idx)]\n",
        "        \n",
        "        # Build movie-tag matrix\n",
        "        unique_movies = tags_df['movie_id'].unique()\n",
        "        movie_to_row = {mid: i for i, mid in enumerate(unique_movies)}\n",
        "        n_movies = len(unique_movies)\n",
        "        \n",
        "        rows, cols, data = [], [], []\n",
        "        movie_tag_counts = tags_df.groupby(['movie_id', 'tag']).size()\n",
        "        \n",
        "        for (movie_id, tag), count in movie_tag_counts.items():\n",
        "            if movie_id in movie_to_row and tag in self.tag_to_idx:\n",
        "                rows.append(movie_to_row[movie_id])\n",
        "                cols.append(self.tag_to_idx[tag])\n",
        "                data.append(count)\n",
        "        \n",
        "        count_matrix = csr_matrix((data, (rows, cols)), shape=(n_movies, n_tags))\n",
        "        \n",
        "        # TF-IDF\n",
        "        row_sums = np.array(count_matrix.sum(axis=1)).flatten()\n",
        "        row_sums[row_sums == 0] = 1\n",
        "        tf = count_matrix.multiply(1.0 / row_sums[:, np.newaxis])\n",
        "        \n",
        "        doc_freq = np.array((count_matrix > 0).sum(axis=0)).flatten()\n",
        "        doc_freq[doc_freq == 0] = 1\n",
        "        idf = np.log(n_movies / doc_freq)\n",
        "        \n",
        "        tfidf = tf.multiply(idf)\n",
        "        \n",
        "        # SVD\n",
        "        k = min(self.n_tag_components, min(n_movies, n_tags) - 1)\n",
        "        if k > 0:\n",
        "            U, s, Vt = svds(tfidf.tocsr(), k=k)\n",
        "            embeddings = U * s\n",
        "            \n",
        "            for movie_id, row_idx in movie_to_row.items():\n",
        "                self.movie_tag_embeddings[movie_id] = embeddings[row_idx].astype(np.float32)\n",
        "        \n",
        "        # Mean embedding for movies without tags\n",
        "        if self.movie_tag_embeddings:\n",
        "            self.mean_tag_embedding = np.mean(\n",
        "                list(self.movie_tag_embeddings.values()), axis=0\n",
        "            ).astype(np.float32)\n",
        "        else:\n",
        "            self.mean_tag_embedding = np.zeros(self.n_tag_components, dtype=np.float32)\n",
        "        \n",
        "        # User tag preferences (weighted average of movie tag embeddings)\n",
        "        user_tag_sums = defaultdict(lambda: np.zeros(self.n_tag_components, dtype=np.float32))\n",
        "        user_tag_counts = defaultdict(float)\n",
        "        \n",
        "        for _, row in train_df.iterrows():\n",
        "            user_id = row['user_id']\n",
        "            movie_id = row['movie_id']\n",
        "            rating = row['rating']\n",
        "            \n",
        "            if movie_id in self.movie_tag_embeddings and rating >= 4.0:\n",
        "                user_tag_sums[user_id] += self.movie_tag_embeddings[movie_id]\n",
        "                user_tag_counts[user_id] += 1\n",
        "        \n",
        "        for user_id, tag_sum in user_tag_sums.items():\n",
        "            count = user_tag_counts[user_id]\n",
        "            if count > 0:\n",
        "                self.user_tag_prefs[user_id] = tag_sum / count\n",
        "    \n",
        "    def get_user_features(self, user_id: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get feature vector for a user.\n",
        "        \n",
        "        Returns ~30 features.\n",
        "        \"\"\"\n",
        "        features = []\n",
        "        \n",
        "        # Basic stats (10 features)\n",
        "        if user_id in self.user_stats:\n",
        "            stats = self.user_stats[user_id]\n",
        "            features.extend([\n",
        "                stats['mean'],\n",
        "                stats['std'],\n",
        "                np.log1p(stats['count']),\n",
        "                stats['min'],\n",
        "                stats['max'],\n",
        "                stats['median'],\n",
        "                stats['q25'],\n",
        "                stats['q75'],\n",
        "                stats['range'],\n",
        "                stats['skew'],\n",
        "            ])\n",
        "        else:\n",
        "            features.extend([self.global_mean, 0.5, 0, 1, 5, self.global_mean, \n",
        "                           2.5, 4.5, 4, 0])\n",
        "        \n",
        "        # Genre preferences (n_genres features)\n",
        "        if user_id in self.user_genre_prefs:\n",
        "            features.extend(self.user_genre_prefs[user_id].tolist())\n",
        "        else:\n",
        "            features.extend([0.0] * len(self.genre_list))\n",
        "        \n",
        "        # Tag preferences (n_tag_components features)\n",
        "        if user_id in self.user_tag_prefs:\n",
        "            features.extend(self.user_tag_prefs[user_id].tolist())\n",
        "        else:\n",
        "            features.extend([0.0] * self.n_tag_components)\n",
        "        \n",
        "        return np.array(features, dtype=np.float32)\n",
        "    \n",
        "    def get_movie_features(self, movie_id: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get feature vector for a movie.\n",
        "        \n",
        "        Returns ~85 features.\n",
        "        \"\"\"\n",
        "        features = []\n",
        "        \n",
        "        # Basic stats (13 features)\n",
        "        if movie_id in self.movie_stats:\n",
        "            stats = self.movie_stats[movie_id]\n",
        "            features.extend([\n",
        "                stats['mean'],\n",
        "                stats['std'],\n",
        "                stats['log_count'],\n",
        "                stats['min'],\n",
        "                stats['max'],\n",
        "                stats['median'],\n",
        "                stats['q25'],\n",
        "                stats['q75'],\n",
        "                stats['range'],\n",
        "                stats['skew'],\n",
        "                stats['popularity_tier'],\n",
        "                stats['is_rare'],\n",
        "                stats['is_popular'],\n",
        "            ])\n",
        "        else:\n",
        "            features.extend([self.global_mean, 0.5, 0, 1, 5, self.global_mean,\n",
        "                           2.5, 4.5, 4, 0, 0, 1, 0])\n",
        "        \n",
        "        # Genre features (n_genres features)\n",
        "        if movie_id in self.movie_genres:\n",
        "            features.extend(self.movie_genres[movie_id].tolist())\n",
        "        else:\n",
        "            features.extend([0.0] * len(self.genre_list))\n",
        "        \n",
        "        # Tag embedding (n_tag_components features)\n",
        "        if movie_id in self.movie_tag_embeddings:\n",
        "            features.extend(self.movie_tag_embeddings[movie_id].tolist())\n",
        "        else:\n",
        "            features.extend(self.mean_tag_embedding.tolist())\n",
        "        \n",
        "        return np.array(features, dtype=np.float32)\n",
        "    \n",
        "    def get_cross_features(self, user_id: int, movie_id: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get cross features for a user-movie pair.\n",
        "        \n",
        "        Returns ~15 features.\n",
        "        \"\"\"\n",
        "        features = []\n",
        "        \n",
        "        # User-movie genre match (dot product)\n",
        "        user_genre = self.user_genre_prefs.get(user_id, np.zeros(len(self.genre_list)))\n",
        "        movie_genre = self.movie_genres.get(movie_id, np.zeros(len(self.genre_list)))\n",
        "        \n",
        "        genre_dot = np.dot(user_genre, movie_genre)\n",
        "        genre_cos = genre_dot / (np.linalg.norm(user_genre) * np.linalg.norm(movie_genre) + 1e-8)\n",
        "        \n",
        "        features.extend([genre_dot, genre_cos])\n",
        "        \n",
        "        # User-movie tag match\n",
        "        user_tag = self.user_tag_prefs.get(user_id, np.zeros(self.n_tag_components))\n",
        "        movie_tag = self.movie_tag_embeddings.get(movie_id, self.mean_tag_embedding)\n",
        "        \n",
        "        tag_dot = np.dot(user_tag, movie_tag)\n",
        "        tag_cos = tag_dot / (np.linalg.norm(user_tag) * np.linalg.norm(movie_tag) + 1e-8)\n",
        "        \n",
        "        features.extend([tag_dot, tag_cos])\n",
        "        \n",
        "        # Rating deviation features\n",
        "        user_mean = self.user_stats.get(user_id, {}).get('mean', self.global_mean)\n",
        "        movie_mean = self.movie_stats.get(movie_id, {}).get('mean', self.global_mean)\n",
        "        \n",
        "        features.extend([\n",
        "            user_mean - self.global_mean,  # User bias\n",
        "            movie_mean - self.global_mean,  # Movie bias\n",
        "            user_mean - movie_mean,  # User-movie bias diff\n",
        "            abs(user_mean - movie_mean),  # Absolute diff\n",
        "        ])\n",
        "        \n",
        "        # User activity vs movie popularity\n",
        "        user_count = self.user_stats.get(user_id, {}).get('count', 0)\n",
        "        movie_count = self.movie_stats.get(movie_id, {}).get('count', 0)\n",
        "        \n",
        "        features.extend([\n",
        "            np.log1p(user_count),\n",
        "            np.log1p(movie_count),\n",
        "            np.log1p(user_count) - np.log1p(movie_count),\n",
        "        ])\n",
        "        \n",
        "        # Variance features\n",
        "        user_std = self.user_stats.get(user_id, {}).get('std', 0.5)\n",
        "        movie_std = self.movie_stats.get(movie_id, {}).get('std', 0.5)\n",
        "        \n",
        "        features.extend([\n",
        "            user_std,\n",
        "            movie_std,\n",
        "            user_std * movie_std,\n",
        "        ])\n",
        "        \n",
        "        return np.array(features, dtype=np.float32)\n",
        "    \n",
        "    def get_all_features(self, user_id: int, movie_id: int) -> np.ndarray:\n",
        "        \"\"\"Get complete feature vector for a user-movie pair.\"\"\"\n",
        "        user_feats = self.get_user_features(user_id)\n",
        "        movie_feats = self.get_movie_features(movie_id)\n",
        "        cross_feats = self.get_cross_features(user_id, movie_id)\n",
        "        \n",
        "        return np.concatenate([user_feats, movie_feats, cross_feats])\n",
        "    \n",
        "    def get_features_batch(\n",
        "        self,\n",
        "        user_ids: np.ndarray,\n",
        "        movie_ids: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Get features for multiple user-movie pairs.\"\"\"\n",
        "        n = len(user_ids)\n",
        "        \n",
        "        # Get feature dimension\n",
        "        sample_feats = self.get_all_features(user_ids[0], movie_ids[0])\n",
        "        n_features = len(sample_feats)\n",
        "        \n",
        "        features = np.zeros((n, n_features), dtype=np.float32)\n",
        "        \n",
        "        for i in range(n):\n",
        "            features[i] = self.get_all_features(user_ids[i], movie_ids[i])\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def get_feature_names(self) -> List[str]:\n",
        "        \"\"\"Get names of all features.\"\"\"\n",
        "        names = []\n",
        "        \n",
        "        # User stats\n",
        "        names.extend([\n",
        "            'user_mean', 'user_std', 'user_log_count', 'user_min', 'user_max',\n",
        "            'user_median', 'user_q25', 'user_q75', 'user_range', 'user_skew'\n",
        "        ])\n",
        "        \n",
        "        # User genre prefs\n",
        "        names.extend([f'user_genre_{g}' for g in self.genre_list])\n",
        "        \n",
        "        # User tag prefs\n",
        "        names.extend([f'user_tag_{i}' for i in range(self.n_tag_components)])\n",
        "        \n",
        "        # Movie stats\n",
        "        names.extend([\n",
        "            'movie_mean', 'movie_std', 'movie_log_count', 'movie_min', 'movie_max',\n",
        "            'movie_median', 'movie_q25', 'movie_q75', 'movie_range', 'movie_skew',\n",
        "            'movie_popularity_tier', 'movie_is_rare', 'movie_is_popular'\n",
        "        ])\n",
        "        \n",
        "        # Movie genres\n",
        "        names.extend([f'movie_genre_{g}' for g in self.genre_list])\n",
        "        \n",
        "        # Movie tag embedding\n",
        "        names.extend([f'movie_tag_{i}' for i in range(self.n_tag_components)])\n",
        "        \n",
        "        # Cross features\n",
        "        names.extend([\n",
        "            'cross_genre_dot', 'cross_genre_cos',\n",
        "            'cross_tag_dot', 'cross_tag_cos',\n",
        "            'user_bias', 'movie_bias', 'bias_diff', 'bias_diff_abs',\n",
        "            'user_log_count', 'movie_log_count', 'count_diff',\n",
        "            'user_std', 'movie_std', 'std_product'\n",
        "        ])\n",
        "        \n",
        "        return names\n",
        "\n",
        "print('\u2713 FeatureEngineer defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Competition Ensemble (Stacked Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class CompetitionEnsemble:\n    \"\"\"\n    Two-level stacked ensemble optimized for Weighted RMSE.\n    \n    Level 1: Base models (SVD, Implicit ALS, Neural CF)\n    Level 2: LightGBM meta-learner with W-RMSE sample weights\n    \"\"\"\n    \n    def __init__(\n        self,\n        use_implicit: bool = True,\n        use_neural: bool = True,\n        n_folds: int = 5,\n        lgbm_params: Optional[Dict] = None,\n        random_state: int = 42,\n        verbose: bool = True\n    ):\n        \"\"\"\n        Initialize competition ensemble.\n        \n        Args:\n            use_implicit: Include Implicit ALS model\n            use_neural: Include Neural CF model\n            n_folds: Number of folds for OOF predictions\n            lgbm_params: LightGBM parameters (or use defaults)\n            random_state: Random seed\n            verbose: Print progress\n        \"\"\"\n        if not LGBM_AVAILABLE and not SKLEARN_GB_AVAILABLE:\n            raise ImportError(\"LightGBM or sklearn required\")\n        \n        self.use_implicit = use_implicit\n        self.use_neural = use_neural\n        self.n_folds = n_folds\n        self.random_state = random_state\n        self.verbose = verbose\n        \n        # Default LightGBM parameters optimized for RMSE\n        self.lgbm_params = lgbm_params or {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting_type': 'gbdt',\n            'num_leaves': 63,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'min_child_samples': 20,\n            'lambda_l1': 0.1,\n            'lambda_l2': 0.1,\n            'verbose': -1,\n            'seed': random_state,\n            'n_jobs': -1,\n        }\n        \n        # Models and components\n        self.base_models = {}\n        self.meta_model = None\n        self.feature_engineer = None\n        \n        # Stored data\n        self.train_df = None\n        self.movie_counts = None\n        self.global_mean = 3.5\n        \n        self.is_fitted = False\n    \n    def _compute_sample_weights(self, movie_ids: np.ndarray) -> np.ndarray:\n        \"\"\"Compute W-RMSE sample weights: w_i = 1 / sqrt(movie_count).\"\"\"\n        weights = np.array([\n            1.0 / np.sqrt(self.movie_counts.get(mid, 1))\n            for mid in movie_ids\n        ])\n        # Normalize\n        weights = weights / weights.sum() * len(weights)\n        return weights\n    \n    def _train_base_models(\n        self,\n        train_df: pd.DataFrame,\n        movies_df: pd.DataFrame,\n        tags_df: Optional[pd.DataFrame],\n        implicit_df: Optional[pd.DataFrame]\n    ):\n        \"\"\"Train all base models.\"\"\"\n        # SVDRecommender already defined in notebook\n        \n        # 1. SVD models with different factors\n        if self.verbose:\n            print(\"\\n[1/4] Training SVD models...\")\n        \n        self.base_models['svd_100'] = SVDRecommender(n_factors=100, random_state=self.random_state)\n        self.base_models['svd_100'].fit(train_df)\n        \n        self.base_models['svd_50'] = SVDRecommender(n_factors=50, random_state=self.random_state)\n        self.base_models['svd_50'].fit(train_df)\n        \n        # 2. Implicit ALS\n        if self.use_implicit and implicit_df is not None:\n            if self.verbose:\n                print(\"\\n[2/4] Training Implicit ALS...\")\n            try:\n        # ImplicitALS not available in this notebook\n                self.base_models['implicit_als'] = ImplicitALS(\n                    n_factors=100,\n                    alpha=40.0,\n                    n_iterations=15,\n                    random_state=self.random_state,\n                    verbose=self.verbose\n                )\n                self.base_models['implicit_als'].fit(train_df, implicit_df)\n            except Exception as e:\n                warnings.warn(f\"Implicit ALS failed: {e}\")\n        \n        # 3. Neural CF\n        if self.use_neural:\n            if self.verbose:\n                print(\"\\n[3/4] Training Neural CF...\")\n            try:\n        # NeuralCF not available in this notebook\n                if TORCH_AVAILABLE:\n                    self.base_models['neural_cf'] = NeuralCF(\n                        gmf_dim=32,\n                        mlp_dims=[64, 32, 16],\n                        n_epochs=15,\n                        batch_size=2048,\n                        random_state=self.random_state,\n                        verbose=self.verbose\n                    )\n                    self.base_models['neural_cf'].fit(train_df)\n            except Exception as e:\n                warnings.warn(f\"Neural CF failed: {e}\")\n        \n        # 4. Feature Engineer\n        if self.verbose:\n            print(\"\\n[4/4] Fitting feature engineer...\")\n        # FeatureEngineer already defined in notebook\n        self.feature_engineer = FeatureEngineer(\n            n_tag_components=50,\n            verbose=self.verbose\n        )\n        self.feature_engineer.fit(train_df, movies_df, tags_df)\n    \n    def _get_base_predictions(\n        self,\n        user_ids: np.ndarray,\n        movie_ids: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"Get predictions from all base models.\"\"\"\n        n = len(user_ids)\n        pairs = list(zip(user_ids, movie_ids))\n        \n        preds = []\n        for name, model in self.base_models.items():\n            try:\n                model_preds = model.predict_batch(pairs)\n                preds.append(model_preds)\n            except Exception as e:\n                warnings.warn(f\"Model {name} prediction failed: {e}\")\n                preds.append(np.full(n, self.global_mean))\n        \n        return np.column_stack(preds)\n    \n    def _generate_oof_predictions(\n        self,\n        train_df: pd.DataFrame,\n        movies_df: pd.DataFrame,\n        tags_df: Optional[pd.DataFrame],\n        implicit_df: Optional[pd.DataFrame]\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Generate out-of-fold predictions for stacking.\n        \n        Returns:\n            (features, base_predictions, targets)\n        \"\"\"\n        # SVDRecommender already defined in notebook\n        \n        n = len(train_df)\n        user_ids = train_df['user_id'].values\n        movie_ids = train_df['movie_id'].values\n        targets = train_df['rating'].values\n        \n        # Initialize arrays for OOF predictions\n        n_base_models = 2  # SVD_100, SVD_50\n        if self.use_implicit:\n            n_base_models += 1\n        if self.use_neural:\n            n_base_models += 1\n        \n        oof_base_preds = np.zeros((n, n_base_models), dtype=np.float32)\n        \n        # Cross-validation for OOF\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n        \n        if self.verbose:\n            print(f\"\\nGenerating OOF predictions ({self.n_folds}-fold)...\")\n        \n        for fold, (train_idx, val_idx) in enumerate(kfold.split(train_df)):\n            if self.verbose:\n                print(f\"\\n--- Fold {fold + 1}/{self.n_folds} ---\")\n            \n            fold_train = train_df.iloc[train_idx]\n            fold_val = train_df.iloc[val_idx]\n            \n            val_user_ids = fold_val['user_id'].values\n            val_movie_ids = fold_val['movie_id'].values\n            val_pairs = list(zip(val_user_ids, val_movie_ids))\n            \n            col = 0\n            \n            # SVD 100\n            svd_100 = SVDRecommender(n_factors=100, random_state=self.random_state)\n            svd_100.fit(fold_train)\n            oof_base_preds[val_idx, col] = svd_100.predict_batch(val_pairs)\n            col += 1\n            \n            # SVD 50\n            svd_50 = SVDRecommender(n_factors=50, random_state=self.random_state)\n            svd_50.fit(fold_train)\n            oof_base_preds[val_idx, col] = svd_50.predict_batch(val_pairs)\n            col += 1\n            \n            # Implicit ALS (if available)\n            if self.use_implicit and implicit_df is not None:\n                try:\n        # ImplicitALS not available in this notebook\n                    impl_als = ImplicitALS(\n                        n_factors=100, alpha=40.0, n_iterations=10,\n                        random_state=self.random_state, verbose=False\n                    )\n                    impl_als.fit(fold_train, implicit_df)\n                    oof_base_preds[val_idx, col] = impl_als.predict_batch(val_pairs)\n                except Exception as e:\n                    oof_base_preds[val_idx, col] = self.global_mean\n                col += 1\n            \n            # Neural CF (if available)\n            if self.use_neural:\n                try:\n        # NeuralCF not available in this notebook\n                    if TORCH_AVAILABLE:\n                        ncf = NeuralCF(\n                            gmf_dim=32, mlp_dims=[64, 32, 16],\n                            n_epochs=10, batch_size=2048,\n                            random_state=self.random_state, verbose=False\n                        )\n                        ncf.fit(fold_train)\n                        oof_base_preds[val_idx, col] = ncf.predict_batch(val_pairs)\n                    else:\n                        oof_base_preds[val_idx, col] = self.global_mean\n                except Exception as e:\n                    oof_base_preds[val_idx, col] = self.global_mean\n                col += 1\n        \n        # Now fit feature engineer on FULL training data\n        if self.verbose:\n            print(\"\\nFitting feature engineer on full data...\")\n        # FeatureEngineer already defined in notebook\n        self.feature_engineer = FeatureEngineer(n_tag_components=50, verbose=False)\n        self.feature_engineer.fit(train_df, movies_df, tags_df)\n        \n        # Extract features for all training samples\n        if self.verbose:\n            print(\"Extracting features...\")\n        features = self.feature_engineer.get_features_batch(user_ids, movie_ids)\n        \n        return features, oof_base_preds, targets\n    \n    def fit(\n        self,\n        train_df: pd.DataFrame,\n        movies_df: pd.DataFrame,\n        tags_df: Optional[pd.DataFrame] = None,\n        implicit_df: Optional[pd.DataFrame] = None,\n        use_oof: bool = True\n    ) -> 'CompetitionEnsemble':\n        \"\"\"\n        Fit the competition ensemble.\n        \n        Args:\n            train_df: Training ratings [user_id, movie_id, rating]\n            movies_df: Movie metadata\n            tags_df: Optional tags\n            implicit_df: Optional implicit interactions\n            use_oof: Use out-of-fold predictions for stacking (recommended)\n        \n        Returns:\n            Self\n        \"\"\"\n        np.random.seed(self.random_state)\n        \n        self.train_df = train_df.copy()\n        self.global_mean = train_df['rating'].mean()\n        self.movie_counts = train_df.groupby('movie_id').size().to_dict()\n        \n        if self.verbose:\n            print(\"=\"*60)\n            print(\"COMPETITION ENSEMBLE TRAINING\")\n            print(\"=\"*60)\n            print(f\"Training samples: {len(train_df):,}\")\n            print(f\"Users: {train_df['user_id'].nunique():,}\")\n            print(f\"Movies: {train_df['movie_id'].nunique():,}\")\n        \n        # Get training data arrays\n        user_ids = train_df['user_id'].values\n        movie_ids = train_df['movie_id'].values\n        targets = train_df['rating'].values\n        \n        if use_oof:\n            # Generate OOF predictions and features\n            features, base_preds, targets = self._generate_oof_predictions(\n                train_df, movies_df, tags_df, implicit_df\n            )\n        else:\n            # Direct training (may overfit)\n            self._train_base_models(train_df, movies_df, tags_df, implicit_df)\n            base_preds = self._get_base_predictions(user_ids, movie_ids)\n            features = self.feature_engineer.get_features_batch(user_ids, movie_ids)\n        \n        # Combine features and base predictions\n        X = np.hstack([base_preds, features])\n        y = targets\n        \n        # Compute sample weights for W-RMSE\n        sample_weights = self._compute_sample_weights(movie_ids)\n        \n        if self.verbose:\n            print(f\"\\nMeta-learner input shape: {X.shape}\")\n            print(f\"Feature breakdown: {base_preds.shape[1]} base preds + {features.shape[1]} features\")\n        \n        # Train meta-learner\n        if LGBM_AVAILABLE:\n            if self.verbose:\n                print(\"\\nTraining LightGBM meta-learner...\")\n            \n            train_data = lgb.Dataset(X, label=y, weight=sample_weights)\n            \n            self.meta_model = lgb.train(\n                self.lgbm_params,\n                train_data,\n                num_boost_round=500,\n                valid_sets=[train_data],\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50, verbose=self.verbose),\n                    lgb.log_evaluation(period=50 if self.verbose else 0)\n                ]\n            )\n            self._use_lgbm = True\n        else:\n            if self.verbose:\n                print(\"\\nTraining sklearn GradientBoosting meta-learner...\")\n            \n            from sklearn.ensemble import GradientBoostingRegressor\n            self.meta_model = GradientBoostingRegressor(\n                n_estimators=200,\n                learning_rate=0.05,\n                max_depth=5,\n                min_samples_split=20,\n                min_samples_leaf=10,\n                subsample=0.8,\n                random_state=self.random_state,\n                verbose=1 if self.verbose else 0\n            )\n            self.meta_model.fit(X, y, sample_weight=sample_weights)\n            self._use_lgbm = False\n        \n        # Re-train base models on full data for inference\n        if use_oof:\n            if self.verbose:\n                print(\"\\nRe-training base models on full data...\")\n            self._train_base_models(train_df, movies_df, tags_df, implicit_df)\n        \n        self.is_fitted = True\n        \n        if self.verbose:\n            print(\"\\n\" + \"=\"*60)\n            print(\"\u2713 COMPETITION ENSEMBLE TRAINING COMPLETE!\")\n            print(\"=\"*60)\n        \n        return self\n    \n    def predict(self, user_id: int, movie_id: int) -> float:\n        \"\"\"Predict rating for a user-movie pair.\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Model must be fitted first\")\n        \n        # Get base predictions\n        pairs = [(user_id, movie_id)]\n        base_preds = []\n        \n        for name, model in self.base_models.items():\n            try:\n                pred = model.predict(user_id, movie_id)\n                base_preds.append(pred)\n            except:\n                base_preds.append(self.global_mean)\n        \n        # Get features\n        features = self.feature_engineer.get_all_features(user_id, movie_id)\n        \n        # Combine\n        X = np.concatenate([base_preds, features]).reshape(1, -1)\n        \n        # Meta-learner prediction\n        if hasattr(self, '_use_lgbm') and self._use_lgbm:\n            pred = self.meta_model.predict(X)[0]\n        else:\n            pred = self.meta_model.predict(X)[0]\n        \n        return float(np.clip(pred, 0.5, 5.0))\n    \n    def predict_batch(self, pairs: List[Tuple[int, int]]) -> np.ndarray:\n        \"\"\"Predict ratings for multiple pairs.\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Model must be fitted first\")\n        \n        n = len(pairs)\n        user_ids = np.array([p[0] for p in pairs])\n        movie_ids = np.array([p[1] for p in pairs])\n        \n        # Get base predictions\n        base_preds = self._get_base_predictions(user_ids, movie_ids)\n        \n        # Get features\n        features = self.feature_engineer.get_features_batch(user_ids, movie_ids)\n        \n        # Combine\n        X = np.hstack([base_preds, features])\n        \n        # Meta-learner predictions\n        predictions = self.meta_model.predict(X)\n        \n        return np.clip(predictions, 0.5, 5.0)\n    \n    def get_feature_importance(self) -> pd.DataFrame:\n        \"\"\"Get feature importance from meta-learner.\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Model must be fitted first\")\n        \n        # Build feature names\n        base_model_names = list(self.base_models.keys())\n        feature_names = base_model_names + self.feature_engineer.get_feature_names()\n        \n        if hasattr(self, '_use_lgbm') and self._use_lgbm:\n            importance = self.meta_model.feature_importance(importance_type='gain')\n        else:\n            importance = self.meta_model.feature_importances_\n        \n        df = pd.DataFrame({\n            'feature': feature_names[:len(importance)],\n            'importance': importance\n        }).sort_values('importance', ascending=False)\n        \n        return df\n\nprint('\u2713 CompetitionEnsemble defined')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Load Data (Run Once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and split data - RUN THIS ONCE\n",
        "loader = DataLoader(DATA_DIR)\n",
        "full_df = loader.load_train_data(explicit_only=True)\n",
        "\n",
        "try:\n",
        "    genre_features = loader.get_genre_features()\n",
        "    movies_df = loader.movies_df\n",
        "except:\n",
        "    genre_features = None\n",
        "    movies_df = None\n",
        "\n",
        "try:\n",
        "    tags_df = loader.load_tags()\n",
        "except:\n",
        "    tags_df = None\n",
        "\n",
        "train_df, val_df, test_df = train_val_test_split(full_df)\n",
        "\n",
        "print(f\"\\n\u2713 Data loaded and ready for experiments!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# \ud83e\uddea EXPERIMENTS (Run Each Cell Independently)\n",
        "\n",
        "**Each experiment below is in its own cell. You can run them in any order, skip them, or re-run if needed.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BASELINE EXPERIMENT\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT: BaselineRecommender\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "baseline = BaselineRecommender()\n",
        "baseline.fit(train_df)\n",
        "\n",
        "val_metrics = evaluate_recommender(baseline, val_df, train_df)\n",
        "print(f\"Validation: W-RMSE={val_metrics['weighted_rmse']:.6f}, RMSE={val_metrics['rmse']:.6f}\")\n",
        "\n",
        "train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "baseline_final = BaselineRecommender()\n",
        "baseline_final.fit(train_val_df)\n",
        "\n",
        "test_metrics = evaluate_recommender(baseline_final, test_df, train_val_df)\n",
        "print(f\"Test: W-RMSE={test_metrics['weighted_rmse']:.6f}, RMSE={test_metrics['rmse']:.6f}\")\n",
        "\n",
        "baseline_result = {\n",
        "    'model': 'BaselineRecommender',\n",
        "    'val_wrmse': val_metrics['weighted_rmse'],\n",
        "    'test_wrmse': test_metrics['weighted_rmse'],\n",
        "    'model_instance': baseline_final\n",
        "}\n",
        "\n",
        "print(\"\\n\u2713 Baseline experiment complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: SVD (Currently Best!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVD EXPERIMENT\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT: SVDRecommender\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "svd = SVDRecommender(n_factors=50, random_state=42)\n",
        "svd.fit(train_df)\n",
        "\n",
        "val_metrics = evaluate_recommender(svd, val_df, train_df)\n",
        "print(f\"Validation: W-RMSE={val_metrics['weighted_rmse']:.6f}, RMSE={val_metrics['rmse']:.6f}\")\n",
        "\n",
        "train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "svd_final = SVDRecommender(n_factors=50, random_state=42)\n",
        "svd_final.fit(train_val_df)\n",
        "\n",
        "test_metrics = evaluate_recommender(svd_final, test_df, train_val_df)\n",
        "print(f\"Test: W-RMSE={test_metrics['weighted_rmse']:.6f}, RMSE={test_metrics['rmse']:.6f}\")\n",
        "\n",
        "svd_result = {\n",
        "    'model': 'SVDRecommender',\n",
        "    'val_wrmse': val_metrics['weighted_rmse'],\n",
        "    'test_wrmse': test_metrics['weighted_rmse'],\n",
        "    'model_instance': svd_final\n",
        "}\n",
        "\n",
        "print(\"\\n\u2713 SVD experiment complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3: FunkSVD (FIXED - Better Parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FUNKSVD EXPERIMENT (FIXED VERSION)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT: FunkSVDRecommender (FIXED)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Fixed parameters: more factors, higher lr, more epochs\n",
        "funksvd = FunkSVDRecommender(\n",
        "    n_factors=100,           # Increased from 50\n",
        "    lr=0.01,                 # Increased from 0.005\n",
        "    reg=0.05,                # Increased regularization\n",
        "    n_epochs=50,             # Increased from 20\n",
        "    batch_size=2048,         # Larger batches\n",
        "    early_stop_patience=10,  # More patience\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "funksvd.fit(train_df)\n",
        "\n",
        "val_metrics = evaluate_recommender(funksvd, val_df, train_df)\n",
        "print(f\"\\nValidation: W-RMSE={val_metrics['weighted_rmse']:.6f}, RMSE={val_metrics['rmse']:.6f}\")\n",
        "\n",
        "train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "funksvd_final = FunkSVDRecommender(\n",
        "    n_factors=100, lr=0.01, reg=0.05, n_epochs=50, \n",
        "    batch_size=2048, val_fraction=0.0,  # No internal validation\n",
        "    random_state=42, verbose=True\n",
        ")\n",
        "funksvd_final.fit(train_val_df)\n",
        "\n",
        "test_metrics = evaluate_recommender(funksvd_final, test_df, train_val_df)\n",
        "print(f\"Test: W-RMSE={test_metrics['weighted_rmse']:.6f}, RMSE={test_metrics['rmse']:.6f}\")\n",
        "\n",
        "funksvd_result = {\n",
        "    'model': 'FunkSVDRecommender',\n",
        "    'val_wrmse': val_metrics['weighted_rmse'],\n",
        "    'test_wrmse': test_metrics['weighted_rmse'],\n",
        "    'model_instance': funksvd_final\n",
        "}\n",
        "\n",
        "print(\"\\n\u2713 FunkSVD experiment complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4: ALS (FIXED - Higher Regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALS EXPERIMENT (FIXED VERSION - was severely overfitting)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT: ALSRecommender (FIXED)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Fixed: MUCH higher regularization to prevent overfitting\n",
        "als = ALSRecommender(\n",
        "    n_factors=50,\n",
        "    reg=0.5,              # INCREASED from 0.1 to 0.5!\n",
        "    n_iterations=10,      # Reduced from 15\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "als.fit(train_df)\n",
        "\n",
        "val_metrics = evaluate_recommender(als, val_df, train_df)\n",
        "print(f\"\\nValidation: W-RMSE={val_metrics['weighted_rmse']:.6f}, RMSE={val_metrics['rmse']:.6f}\")\n",
        "\n",
        "train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "als_final = ALSRecommender(\n",
        "    n_factors=50, reg=0.5, n_iterations=10,\n",
        "    random_state=42, verbose=True\n",
        ")\n",
        "als_final.fit(train_val_df)\n",
        "\n",
        "test_metrics = evaluate_recommender(als_final, test_df, train_val_df)\n",
        "print(f\"Test: W-RMSE={test_metrics['weighted_rmse']:.6f}, RMSE={test_metrics['rmse']:.6f}\")\n",
        "\n",
        "als_result = {\n",
        "    'model': 'ALSRecommender',\n",
        "    'val_wrmse': val_metrics['weighted_rmse'],\n",
        "    'test_wrmse': test_metrics['weighted_rmse'],\n",
        "    'model_instance': als_final\n",
        "}\n",
        "\n",
        "print(\"\\n\u2713 ALS experiment complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 5: Hybrid (CF + Content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HYBRID EXPERIMENT\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT: HybridRecommender\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "if genre_features is None:\n",
        "    print(\"\u26a0 Genre features not available, skipping Hybrid\")\n",
        "    hybrid_result = {'model': 'HybridRecommender', 'val_wrmse': float('inf'), 'test_wrmse': float('inf')}\n",
        "else:\n",
        "    hybrid = HybridRecommender(\n",
        "        n_factors=50,\n",
        "        lr=0.005,\n",
        "        reg=0.02,\n",
        "        n_epochs=30,\n",
        "        genre_features=genre_features,\n",
        "        random_state=42,\n",
        "        verbose=True\n",
        "    )\n",
        "    hybrid.fit(train_df)\n",
        "\n",
        "    val_metrics = evaluate_recommender(hybrid, val_df, train_df)\n",
        "    print(f\"\\nValidation: W-RMSE={val_metrics['weighted_rmse']:.6f}, RMSE={val_metrics['rmse']:.6f}\")\n",
        "\n",
        "    train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    hybrid_final = HybridRecommender(\n",
        "        n_factors=50, lr=0.005, reg=0.02, n_epochs=30,\n",
        "        val_fraction=0.0,  # No internal validation\n",
        "        genre_features=genre_features, random_state=42, verbose=True\n",
        "    )\n",
        "    hybrid_final.fit(train_val_df)\n",
        "\n",
        "    test_metrics = evaluate_recommender(hybrid_final, test_df, train_val_df)\n",
        "    print(f\"Test: W-RMSE={test_metrics['weighted_rmse']:.6f}, RMSE={test_metrics['rmse']:.6f}\")\n",
        "\n",
        "    hybrid_result = {\n",
        "        'model': 'HybridRecommender',\n",
        "        'val_wrmse': val_metrics['weighted_rmse'],\n",
        "        'test_wrmse': test_metrics['weighted_rmse'],\n",
        "        'model_instance': hybrid_final\n",
        "    }\n",
        "\n",
        "print(\"\\n\u2713 Hybrid experiment complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 6: Competition Ensemble (Best Shot!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# COMPETITION ENSEMBLE EXPERIMENT\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT: CompetitionEnsemble\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "if movies_df is None:\n",
        "    print(\"\u26a0 Movies data not available, skipping Ensemble\")\n",
        "    ensemble_result = {'model': 'CompetitionEnsemble', 'val_wrmse': float('inf'), 'test_wrmse': float('inf')}\n",
        "else:\n",
        "    train_val_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "\n",
        "    ensemble = CompetitionEnsemble(\n",
        "        n_folds=3,  # Start with 3 for speed\n",
        "        use_implicit=False,  # Disable for simplicity\n",
        "        use_neural=False,    # Disable for simplicity  \n",
        "        verbose=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    ensemble.fit(train_val_df, movies_df, tags_df, use_oof=True)\n",
        "\n",
        "    # Evaluate on test\n",
        "    pairs = list(zip(test_df['user_id'], test_df['movie_id']))\n",
        "    preds = ensemble.predict_batch(pairs)\n",
        "\n",
        "    test_wrmse = compute_weighted_rmse(\n",
        "        test_df['rating'].values, preds, \n",
        "        test_df['movie_id'].values, train_val_df\n",
        "    )\n",
        "    test_rmse = compute_rmse(test_df['rating'].values, preds)\n",
        "\n",
        "    print(f\"\\nTest: W-RMSE={test_wrmse:.6f}, RMSE={test_rmse:.6f}\")\n",
        "\n",
        "    ensemble_result = {\n",
        "        'model': 'CompetitionEnsemble',\n",
        "        'val_wrmse': test_wrmse,  # Using test as proxy\n",
        "        'test_wrmse': test_wrmse,\n",
        "        'model_instance': ensemble\n",
        "    }\n",
        "\n",
        "    # Show feature importance\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TOP 20 MOST IMPORTANT FEATURES\")\n",
        "    print(\"=\"*60)\n",
        "    print(ensemble.get_feature_importance().head(20).to_string(index=False))\n",
        "\n",
        "print(\"\\n\u2713 Ensemble experiment complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## \ud83d\udcca FINAL RESULTS SUMMARY\n",
        "\n",
        "**Run this cell after completing your experiments to see the final comparison.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RESULTS SUMMARY\n",
        "# Collect all results\n",
        "all_results = []\n",
        "\n",
        "# Add results from experiments that were run\n",
        "if 'baseline_result' in globals():\n",
        "    all_results.append(baseline_result)\n",
        "if 'svd_result' in globals():\n",
        "    all_results.append(svd_result)\n",
        "if 'funksvd_result' in globals():\n",
        "    all_results.append(funksvd_result)\n",
        "if 'als_result' in globals():\n",
        "    all_results.append(als_result)\n",
        "if 'hybrid_result' in globals():\n",
        "    all_results.append(hybrid_result)\n",
        "if 'ensemble_result' in globals():\n",
        "    all_results.append(ensemble_result)\n",
        "\n",
        "if not all_results:\n",
        "    print(\"\u26a0 No experiments completed yet. Run experiment cells above first.\")\n",
        "else:\n",
        "    # Sort by test W-RMSE\n",
        "    sorted_results = sorted(all_results, key=lambda x: x['test_wrmse'])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"{'FINAL RESULTS SUMMARY':^80}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(f\"{'Rank':<6}{'Model':<30}{'Val W-RMSE':<15}{'Test W-RMSE':<15}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for rank, r in enumerate(sorted_results, 1):\n",
        "        marker = \"\ud83c\udfc6\" if rank == 1 else \"  \"\n",
        "        print(f\"{marker} {rank:<4}{r['model']:<30}{r['val_wrmse']:<15.6f}{r['test_wrmse']:<15.6f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"\ud83c\udfc6 BEST MODEL: {sorted_results[0]['model']}\")\n",
        "    print(f\"   Test W-RMSE: {sorted_results[0]['test_wrmse']:.6f}\")\n",
        "\n",
        "    # Improvement over baseline\n",
        "    baseline_score = next((r['test_wrmse'] for r in all_results if r['model'] == 'BaselineRecommender'), None)\n",
        "    if baseline_score:\n",
        "        best_score = sorted_results[0]['test_wrmse']\n",
        "        improvement = baseline_score - best_score\n",
        "        pct_improvement = (improvement / baseline_score) * 100\n",
        "        print(f\"   Improvement over baseline: {improvement:.6f} ({pct_improvement:.2f}%)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"\u2713 All experiments complete!\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Generate Submission\n",
        "\n",
        "**Use your best model to generate the submission file.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate submission with best model\n",
        "# Change 'svd_final' to your best model (e.g., ensemble, hybrid_final, etc.)\n",
        "\n",
        "if 'svd_final' in globals():\n",
        "    best_model = svd_final  # Change this to your best model\n",
        "    \n",
        "    submission_df = loader.load_submission_template()\n",
        "    pairs = list(zip(submission_df['user_id'], submission_df['movie_id']))\n",
        "    predictions = np.clip(best_model.predict_batch(pairs), 0.5, 5.0)\n",
        "    \n",
        "    submission = pd.DataFrame({\n",
        "        'id': submission_df['id'],\n",
        "        'prediction': predictions\n",
        "    })\n",
        "    \n",
        "    output_path = f\"submission_{best_model.__class__.__name__}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    submission.to_csv(output_path, index=False)\n",
        "    \n",
        "    print(f\"\u2713 Submission saved to: {output_path}\")\n",
        "    print(f\"  Mean: {predictions.mean():.4f}, Std: {predictions.std():.4f}\")\n",
        "    \n",
        "    # Download in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(output_path)\n",
        "    except:\n",
        "        print(f\"  File saved locally: {output_path}\")\n",
        "else:\n",
        "    print(\"\u26a0 No model available yet. Run experiments first!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}